{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txu2Dq7ZJ6lf"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR6CmnjyJ9jm"
      },
      "source": [
        "For this project we have chosen to use google colab to run our code. The reason is that free gpu is provided which we can use in the later part of this report to speed up our GNN learning process. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Axdsv02EA6O"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order for this notebook to work you have to commit to the following steps : \n",
        "\n",
        "1. Download the project folder from our [github page](https://github.com/Potamitisn/NML_Project) and upload it to your Google Drive.\n",
        "\n",
        "2. In the following code block change the `project_path` to the path leading where you saved you the project folder.\n",
        "\n",
        "3. You're good to go !"
      ],
      "metadata": {
        "id": "cQrUFekHOQeO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAPXio8jEaNa"
      },
      "outputs": [],
      "source": [
        "project_path = '/content/drive/MyDrive/Colab Notebooks/network_machine_learning/project'\n",
        "! cd project_path\n",
        "\n",
        "import os\n",
        "os.chdir(project_path)\n",
        "current_path = os.getcwd()\n",
        "\n",
        "figures_path = os.path.join(current_path, 'figures')\n",
        "images_path = os.path.join(current_path, 'images')\n",
        "gexf_path = os.path.join(current_path, 'gexf_files')\n",
        "graphs_path = os.path.join(current_path, 'graphs')\n",
        "features_path = os.path.join(current_path, 'features')\n",
        "\n",
        "if not os.path.exists(gexf_path):\n",
        "    os.makedirs(gexf_path)\n",
        "\n",
        "if not os.path.exists(graphs_path):\n",
        "    os.makedirs(graphs_path)\n",
        "\n",
        "if not os.path.exists(features_path):\n",
        "    os.makedirs(features_path)\n",
        "\n",
        "if not os.path.exists(figures_path):\n",
        "    os.makedirs(figures_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXQH0j4kKSJL"
      },
      "outputs": [],
      "source": [
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
        "\n",
        "!pip install torchmetrics\n",
        "\n",
        "from IPython.display import clear_output \n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoPasCnOEuRB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy\n",
        "import networkx as nx\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkfxbNuqKUdH"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, List, Optional\n",
        "import torch\n",
        "import torchmetrics\n",
        "import torch_geometric as pyg\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch_geometric.data import Dataset, Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.datasets import GNNBenchmarkDataset, Planetoid\n",
        "from torch_geometric.utils import from_networkx, to_networkx, get_laplacian\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from tqdm.notebook import tqdm\n",
        "from torch_geometric.utils.convert import from_networkx\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGaTmD03QJqh"
      },
      "outputs": [],
      "source": [
        "!pip install stellargraph\n",
        "from gensim.models import Word2Vec\n",
        "from stellargraph.data import BiasedRandomWalk\n",
        "from stellargraph import StellarGraph\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CINqEFYmldeB"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# additional imports are necessary\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE, Isomap\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1fu9rs8ZYUA"
      },
      "outputs": [],
      "source": [
        "from plotly.subplots import make_subplots\n",
        "from PIL import Image\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lre7DxfEROb"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwGrRZBUMGWT"
      },
      "source": [
        "### Importing and displaying the datasets\n",
        "\n",
        "In the beginning we will import our data into a `pandas.DataFrame`structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uEicSQpESK6"
      },
      "outputs": [],
      "source": [
        "DC_data_2000 = pd.read_parquet('data/dc_2000-2001.parquet', engine='pyarrow')\n",
        "DC_data_2001 = pd.read_parquet('data/dc_2001-2002.parquet', engine='pyarrow')\n",
        "\n",
        "marvel_data_2000 = pd.read_parquet('data/marvel_2000-2001.parquet', engine='pyarrow')\n",
        "marvel_data_2001 = pd.read_parquet('data/marvel_2001-2002.parquet', engine='pyarrow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yBIGTBTMXcT"
      },
      "source": [
        "The next set will be to sort our dataframe in ascending alphabetical order based on the `URL`column, reset `the index` and create a new column named `Id` whose values are copied from the `index` column. Finally, we will display a portion of the dataset to get a glimpse.\n",
        "\n",
        "- We will repeat this process for each of the 4 datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJDJiBFoNfDJ"
      },
      "source": [
        "**DC 2000**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_vSi9CWW1Lw"
      },
      "outputs": [],
      "source": [
        "DC_data_2000 = DC_data_2000.sort_values(by='URL').reset_index()\n",
        "DC_data_2000 = DC_data_2000.reset_index().rename(columns={\"index\": \"Id\"})\n",
        "DC_data_2000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV2_JawcNiKH"
      },
      "source": [
        "**DC 2001**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOipg_5RExMJ"
      },
      "outputs": [],
      "source": [
        "DC_data_2001 = DC_data_2001.sort_values(by='URL').reset_index()\n",
        "DC_data_2001 = DC_data_2001.reset_index().rename(columns={\"index\": \"Id\"})\n",
        "DC_data_2001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4pU4NAINkuK"
      },
      "source": [
        "**Marvel 2000**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKX-LJgGW6IK"
      },
      "outputs": [],
      "source": [
        "marvel_data_2000 = marvel_data_2000.sort_values(by='URL').reset_index()\n",
        "marvel_data_2000 = marvel_data_2000.reset_index().rename(columns={\"index\": \"Id\"})\n",
        "marvel_data_2000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbz3DZCgNm4d"
      },
      "source": [
        "**Marvel 2001**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lAd8PswW6Ol"
      },
      "outputs": [],
      "source": [
        "marvel_data_2001 = marvel_data_2001.sort_values(by='URL').reset_index()\n",
        "marvel_data_2001 = marvel_data_2001.reset_index().rename(columns={\"index\": \"Id\"})\n",
        "marvel_data_2001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lovyf3q9XJ3e"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naP0bGL6XSiX"
      },
      "source": [
        "In order to get a better understanding of our datasets we will use the `.info()`function to check the type of the values for each column and the number of null/non-null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Egw0jfauXVZX"
      },
      "outputs": [],
      "source": [
        "DC_data_2000.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYf4ZvWgXVcM"
      },
      "outputs": [],
      "source": [
        "DC_data_2001.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43YsBW-aXVfb"
      },
      "outputs": [],
      "source": [
        "marvel_data_2000.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgJP_bamXc7v"
      },
      "outputs": [],
      "source": [
        "marvel_data_2001.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMLEumhtXfCk"
      },
      "source": [
        "From above printing, we can see there is no missing value for features in the datasets. However, when we displayed the datasets earlier we have noticed that in the **Relatives** and **Affiliation** features/columns there were instead some empty lists of the form `[]`. In the following, we will take a look into each dataset and check the number of times there is an empty list in either of these features or even **Comics** too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjzKnwISOAg0"
      },
      "outputs": [],
      "source": [
        "empty_relatives_marvel_2000 = marvel_data_2000[marvel_data_2000['Relatives'].apply(lambda x: len(x)==0)]\n",
        "empty_affliation_marvel_2000 = marvel_data_2000[marvel_data_2000['Affiliation'].apply(lambda x: len(x)==0)]\n",
        "empty_comics_marvel_2000 = marvel_data_2000[marvel_data_2000['Comics'].apply(lambda x: len(x)==0)]\n",
        "print('For marvel 2000 dataset, there are {} empty lists in Relatives feature, {} empty lists in Affliation feature and {} empty lists in Comics feature.'.format(len(empty_relatives_marvel_2000), len(empty_affliation_marvel_2000), len(empty_comics_marvel_2000)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd25eI4iOAmK"
      },
      "outputs": [],
      "source": [
        "empty_relatives_marvel_2001 = marvel_data_2001[marvel_data_2001['Relatives'].apply(lambda x: len(x)==0)]\n",
        "empty_affliation_marvel_2001 = marvel_data_2001[marvel_data_2001['Affiliation'].apply(lambda x: len(x)==0)]\n",
        "empty_comics_marvel_2001 = marvel_data_2001[marvel_data_2001['Comics'].apply(lambda x: len(x)==0)]\n",
        "print('For marvel 2001 dataset, there are {} empty lists in Relatives feature, {} empty lists in Affliation feature and {} empty lists in Comics feature.'.format(len(empty_relatives_marvel_2001), len(empty_affliation_marvel_2001), len(empty_comics_marvel_2001)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af695pSUXViG"
      },
      "outputs": [],
      "source": [
        "empty_relatives_DC_2000 = DC_data_2000[DC_data_2000['Relatives'].apply(lambda x: len(x)==0)]\n",
        "empty_affliation_DC_2000 = DC_data_2000[DC_data_2000['Affiliation'].apply(lambda x: len(x)==0)]\n",
        "empty_comics_DC_2000 = DC_data_2000[DC_data_2000['Comics'].apply(lambda x: len(x)==0)]\n",
        "print('For DC 2000 dataset, there are {} empty lists in Relatives feature, {} empty lists in Affliation feature and {} empty lists in Comics feature.'.format(len(empty_relatives_DC_2000), len(empty_affliation_DC_2000), len(empty_comics_DC_2000)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lte5PyhHXVkp"
      },
      "outputs": [],
      "source": [
        "empty_relatives_DC_2001 = DC_data_2001[DC_data_2001['Relatives'].apply(lambda x: len(x)==0)]\n",
        "empty_affliation_DC_2001 = DC_data_2001[DC_data_2001['Affiliation'].apply(lambda x: len(x)==0)]\n",
        "empty_comics_DC_2001 = DC_data_2001[DC_data_2001['Comics'].apply(lambda x: len(x)==0)]\n",
        "print('For DC 2001 dataset, there are {} empty lists in Relatives feature, {} empty lists in Affliation feature and {} empty lists in Comics feature.'.format(len(empty_relatives_DC_2001), len(empty_affliation_DC_2001), len(empty_comics_DC_2001)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGx_htqaNWL8"
      },
      "source": [
        "From above printings, we can see not all characters in Marvel and DC universe are involved in an Affiliation or have their own Relatives. However, all characters appeared in at least one Comic. Therefore, if we create the graph based on Affiliations and Relatives, not all characters have edges. If we create the graph based on Comics, all characters will be connected.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDd2YMTSBQjz"
      },
      "source": [
        "# Network creation\n",
        "\n",
        "In this section, we consider create network based on **Affiliations**, **Relatives** and **Comics**. Considering above findings, we will normalize all the adjacency matrics to the same range of edge weight, and give small weight to Combics adjacency matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCQA53c_PLFX"
      },
      "source": [
        "## Utility Functions\n",
        "\n",
        "To do this in the next block we will define some utility functions that will help us in this process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKffMh_WTyLp"
      },
      "outputs": [],
      "source": [
        "def get_edge_dataframe(df, edge_name):\n",
        "    \"\"\"\n",
        "    Inputs\n",
        "      df [pandas.DataFrame]: Initial dataframe\n",
        "      edge_name [str]: Name of the column of the df that we want to use as edges to\n",
        "                       create our network. For example Affiliations or Relatives.\n",
        "    Outputs\n",
        "      edge_df [pandas.DataFrame]: This dataframe will connect each pair of characters\n",
        "                                  that share the same edge_name. Each row will portray\n",
        "        the ids and the URLs of each character along with 1 edge_name eg. Affiliation, \n",
        "        which they share. Additionaly, a new column will be added called weight. It \n",
        "        will be of type = \"int\" and its value will be equal to the amount of affiliations \n",
        "        shared between the 2 characters. Output will have the following form : \n",
        "                  \n",
        "                    Id_x|Id_y|weight|URL_x|edge_name|URL_y\n",
        "\n",
        "    \"\"\"\n",
        "    df_source = df[['Id', 'URL', edge_name]].explode(edge_name)\n",
        "    #df_source = df_source.dropna(subset=[edge_name]) # drop empty edge_name, such as affiliation, relatives and Comics\n",
        "    df_target = df_source.copy()\n",
        "\n",
        "    edge_list = df_source.merge(df_target, on=edge_name)\n",
        "    # remove self-loop edges\n",
        "    index_names = edge_list[edge_list.URL_x == edge_list.URL_y].index\n",
        "    edge_list.drop(index_names, inplace = True)\n",
        "\n",
        "    # calculate weight of each edge based on the number of affiliations\n",
        "    weight_list = edge_list.groupby(['Id_x', 'Id_y']).count()[edge_name].reset_index()\n",
        "    weight_list = weight_list.rename(columns={edge_name: \"weight\"})\n",
        "    #weight_list['weight'] = weight_list['weight'].apply(weight_function, kernel_width=10)\n",
        "    #print(len(edge_list), len(weight_list))\n",
        "    edge_df = weight_list.merge(edge_list, on=['Id_x', 'Id_y'], how='inner')\n",
        "\n",
        "    return edge_df\n",
        "\n",
        "def check_symmetric(adjacency):\n",
        "    \"\"\"\n",
        "    Recieves a matrix (in this case it will be the adjacency matrix) and prints\n",
        "    if the given matrix is symmetric or not.\n",
        "\n",
        "    Inputs\n",
        "      adjacency [numpy.array] : Matrix to be checked.\n",
        "    \"\"\"\n",
        "    num_non_symmetric = np.count_nonzero((adjacency != adjacency.T).astype(int))\n",
        "\n",
        "    if num_non_symmetric == 0:\n",
        "      print('This matrix is symmetric!')\n",
        "    else:\n",
        "      print('This matrix is not symmetric!')\n",
        "\n",
        "def create_adjacency_matirx(df):\n",
        "    \"\"\"\n",
        "    Recieves a dataframe which is already processed in a way that each row\n",
        "    is an edge connecting a pair of characters and computes its asjacency matrix\n",
        "    based on the weight (column) of the edge (row).\n",
        "\n",
        "    Inputs\n",
        "      df [pandas.DataFrame] : Dataframe computed using get_edge_dataframe function\n",
        "    \n",
        "    Outputs\n",
        "      adjacency [numpy.array] : Adjacency matrix\n",
        "    \"\"\"\n",
        "    n_nodes = df.Id_x.max() + 1\n",
        "    adjacency = np.zeros((n_nodes, n_nodes), dtype=int)\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        if np.isnan(row.Id_x) or np.isnan(row.Id_y):\n",
        "            continue\n",
        "        \n",
        "        i, j, w = int(row.Id_x), int(row.Id_y), int(row.weight)\n",
        "        adjacency[i, j] = w\n",
        "\n",
        "    return adjacency\n",
        "\n",
        "def normalize_adjacency_matrix(A):\n",
        "    \"\"\"\n",
        "    Min-Max normalization of the entries of the adjacency matrix to the [1,2] range\n",
        "\n",
        "    Inputs \n",
        "      A [numpy.array] : Adjacency matrix\n",
        "    Outputs\n",
        "      A [numpy.array] : Normalized adjacency matrix\n",
        "    \"\"\"\n",
        "    A_nonzero = A[A != 0]\n",
        "    A[A != 0] = ((A_nonzero - A_nonzero.min()) / (A_nonzero.max() - A_nonzero.min())) + 1\n",
        "\n",
        "    return A\n",
        "\n",
        "def set_nodes_attributes(df, graph):\n",
        "    \"\"\"\n",
        "    Given a graph it sets the attributes for each of the graph's nodes. Since in \n",
        "    this case the nodes of our graph represent character in the MC or DC universe\n",
        "    the attributes of each node will be their URL, Real Name, Current Alias, Good\n",
        "    and nb_appearences\n",
        "    \"\"\"\n",
        "    nodes = df[['URL', 'Real Name', 'Current Alias', 'Good', 'nb_appearences']]\n",
        "    node_props = nodes.to_dict()\n",
        "\n",
        "    for key in node_props:\n",
        "        nx.set_node_attributes(graph, node_props[key], key)\n",
        "\n",
        "def visualisation_download(adjacency, adjacency_name, attributes, save_path):\n",
        "    \"\"\"\n",
        "    Recieves an adjacency matrix that will firstly convert to a graph, set its \n",
        "    attributes and then download it as a gexf file\n",
        "    \n",
        "    Inputs \n",
        "      adjacency [numpy.array] : Adjacency matrix\n",
        "      adjacency_name [str] : How to name the gexf file\n",
        "      attributes [pandas.DataFrame] : Dataframe containing the attributes of each node\n",
        "      save_path [str] : Path to the folder where the gexf files should be downloaded to\n",
        "    \"\"\"\n",
        "    graph = nx.from_numpy_array(adjacency)\n",
        "    set_nodes_attributes(attributes, graph)\n",
        "    nx.write_gexf(graph, save_path+'/{}.gexf'.format(adjacency_name))\n",
        "\n",
        "def plot_graphs(graph_name, load_path):\n",
        "    \"\"\"\n",
        "    Display the images (already saved in a folder) for each graph\n",
        "\n",
        "    Inputs\n",
        "      graph_name [str] : Should be one of the following [\"marvel_2000\", \"marvel_2001\"\n",
        "                                                         \"DC_2000\", \"DC_2001\"]\n",
        "      load_path [str] : Path to the location of the folder where the images are \n",
        "                        stored\n",
        "    \"\"\"\n",
        "    fig = make_subplots(\n",
        "      rows=1, cols=2,\n",
        "      subplot_titles=(\"Affiliation Graph\", \"Relatives Graph\"))\n",
        "\n",
        "    im1 = Image.open(load_path + \"/{}_aff.png\".format(graph_name))\n",
        "    im2 = Image.open(load_path + \"/{}_rel.png\".format(graph_name))\n",
        "    fig.add_trace(px.imshow(im1).data[0],row=1, col=1)\n",
        "    fig.add_trace(px.imshow(im2).data[0],row=1, col=2)\n",
        "    fig.show()\n",
        "    del im1, im2\n",
        "\n",
        "    fig = make_subplots(\n",
        "        rows=1, cols=2,\n",
        "        subplot_titles=(\"Comics Graph\", \"Final Graph\"))\n",
        "    im3 = Image.open(load_path + \"/{}_com.png\".format(graph_name))\n",
        "    im4 = Image.open(load_path + \"/{}_whole.png\".format(graph_name))\n",
        "    fig.add_trace(px.imshow(im3).data[0],row=1, col=1)\n",
        "    fig.add_trace(px.imshow(im4).data[0],row=1, col=2)\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDP8lSjcvq_t"
      },
      "source": [
        "## Network creation for each dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpzjYWpTYYZI"
      },
      "source": [
        "For this part, we get the edges respectively based on Affiliation, Relatives and Comics. Then, we also normalize the adjacency matrices to range $[1, 2]$. We use Min-max normalization. We use the range $[1, 2]$ to make sure the smallest edge weight in original adjacency matrix is at least $1$. The formula is as following:\n",
        "\n",
        "$$\n",
        "A_{norm} = \\frac{A - A_{min}}{A_{max} - A_{min}} + 1\n",
        "$$\n",
        "\n",
        "where $A_{norm}$ is the normalized adjacency matrix, $A$ is the original adjacency matrix.\n",
        "\n",
        "We also set the weights of Relatives adjacency matrix as $0.5$ and Comics adjacency matrix as $0.1$, because the Affiliation adjacency matrix can provide the most meaningful information. Good characters always have common affiliations. Also, bad characters have common affiliations. Relatives adjacency matrix can provide meaningful information about the relationships between characters. Comics provide the least information to determine whether a character is good, neutral or bad. We concluded to this information after trying different numbers, using just 1 adjacency matrix and performing the classification tasks at the end of this report.\n",
        "\n",
        "The formula is as following:\n",
        "\n",
        "$$\n",
        "A_{all} = A_{affiliation} + 0.5 * A_{relatives} + 0.1 * A_{comics}\n",
        "$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXCUk9nT1W99"
      },
      "source": [
        "**Create network for marvel 2000**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULAc2Krp1VV1"
      },
      "outputs": [],
      "source": [
        "# Creating dataframe whose rows connect a pair of characters sharing the same affiliation\n",
        "df_edge_list_mar_2000_aff = get_edge_dataframe(marvel_data_2000, edge_name='Affiliation')\n",
        "# Computing the adjacency matrix\n",
        "adjacency_mar_2000_aff = create_adjacency_matirx(df_edge_list_mar_2000_aff)\n",
        "# Normalizing the adjacency matrix\n",
        "adjacency_mar_2000_aff = normalize_adjacency_matrix(adjacency_mar_2000_aff)\n",
        "\n",
        "# Creating dataframe whose rows connect a pair of characters sharing the same relative\n",
        "df_edge_list_mar_2000_rel = get_edge_dataframe(marvel_data_2000, edge_name='Relatives')\n",
        "# Computing the adjacency matrix\n",
        "adjacency_mar_2000_rel = create_adjacency_matirx(df_edge_list_mar_2000_rel)\n",
        "# Normalizing the adjacency matrix\n",
        "adjacency_mar_2000_rel = normalize_adjacency_matrix(adjacency_mar_2000_rel)\n",
        "\n",
        "# Creating dataframe whose rows connect a pair of characters sharing the same comic \n",
        "df_edge_list_mar_2000_com = get_edge_dataframe(marvel_data_2000, edge_name='Comics')\n",
        "# Computing the adjacency matrix\n",
        "adjacency_mar_2000_com = create_adjacency_matirx(df_edge_list_mar_2000_com)\n",
        "# Normalizing the adjacency matrix\n",
        "adjacency_mar_2000_com = normalize_adjacency_matrix(adjacency_mar_2000_com)\n",
        "\n",
        "# Addition of the matrices according to the previously discussed formula \n",
        "adjacency_mar_2000 = adjacency_mar_2000_aff + 0.5 * adjacency_mar_2000_rel + 0.1 * adjacency_mar_2000_com\n",
        "\n",
        "# Creating the graph based on the final adjacency matrix\n",
        "graph_mar_2000 = nx.from_numpy_array(adjacency_mar_2000)\n",
        "\n",
        "# Setting the node attributes for our graph\n",
        "set_nodes_attributes(marvel_data_2000, graph_mar_2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs_TAozH3kLV"
      },
      "outputs": [],
      "source": [
        "graph_mar_2000.nodes[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxcECF1_f14b"
      },
      "outputs": [],
      "source": [
        "visualisation_download(adjacency_mar_2000_aff, \"adjacency_mar_2000_aff\", marvel_data_2000, gexf_path)\n",
        "visualisation_download(adjacency_mar_2000_rel, \"adjacency_mar_2000_rel\", marvel_data_2000, gexf_path)\n",
        "visualisation_download(adjacency_mar_2000_com, \"adjacency_mar_2000_com\", marvel_data_2000, gexf_path)\n",
        "visualisation_download(adjacency_mar_2000, \"adjacency_mar_2000\", marvel_data_2000, gexf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ROuWWMzGhmbn"
      },
      "outputs": [],
      "source": [
        "plot_graphs(\"marvel_2000\", images_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a more detailed view of the final graph for Marvel 2000 you can visit [our interactive page](https://potamitisn.github.io/NML_Project/Networks/Marvel_2000/)"
      ],
      "metadata": {
        "id": "LKCogHN_tsih"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjUhDHbm3Pdd"
      },
      "source": [
        "**Create network for marvel 2001**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLMF7LP13SEb"
      },
      "outputs": [],
      "source": [
        "df_edge_list_mar_2001_aff = get_edge_dataframe(marvel_data_2001, edge_name='Affiliation')\n",
        "adjacency_mar_2001_aff = create_adjacency_matirx(df_edge_list_mar_2001_aff)\n",
        "adjacency_mar_2001_aff = normalize_adjacency_matrix(adjacency_mar_2001_aff)\n",
        "\n",
        "df_edge_list_mar_2001_rel = get_edge_dataframe(marvel_data_2001, edge_name='Relatives')\n",
        "adjacency_mar_2001_rel = create_adjacency_matirx(df_edge_list_mar_2001_rel)\n",
        "adjacency_mar_2001_rel = normalize_adjacency_matrix(adjacency_mar_2001_rel)\n",
        "\n",
        "df_edge_list_mar_2001_com = get_edge_dataframe(marvel_data_2001, edge_name='Comics')\n",
        "adjacency_mar_2001_com = create_adjacency_matirx(df_edge_list_mar_2001_com)\n",
        "adjacency_mar_2001_com = normalize_adjacency_matrix(adjacency_mar_2001_com)\n",
        "\n",
        "adjacency_mar_2001 = adjacency_mar_2001_aff + 0.5 * adjacency_mar_2001_rel + 0.1 * adjacency_mar_2001_com\n",
        "\n",
        "graph_mar_2001 = nx.from_numpy_array(adjacency_mar_2001)\n",
        "\n",
        "set_nodes_attributes(marvel_data_2001, graph_mar_2001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du3CwYue37qV"
      },
      "outputs": [],
      "source": [
        "graph_mar_2001.nodes[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualisation_download(adjacency_mar_2001_aff, \"adjacency_mar_2001_aff\", marvel_data_2001, gexf_path)\n",
        "visualisation_download(adjacency_mar_2001_rel, \"adjacency_mar_2001_rel\", marvel_data_2001, gexf_path)\n",
        "visualisation_download(adjacency_mar_2001_com, \"adjacency_mar_2001_com\", marvel_data_2001, gexf_path)\n",
        "visualisation_download(adjacency_mar_2001, \"adjacency_mar_2001\", marvel_data_2001, gexf_path)"
      ],
      "metadata": {
        "id": "BxGqAhUbkRGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graphs(\"marvel_2001\", images_path)"
      ],
      "metadata": {
        "id": "3iP3UGQzlVba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a more detailed view of the final graph for Marvel 2001 you can visit [our interactive page](https://potamitisn.github.io/NML_Project/Networks/Marvel_2001/)"
      ],
      "metadata": {
        "id": "hCunhqUTCYVl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FdDevTJ2euN"
      },
      "source": [
        "**Create network for DC 2000**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6cNCeKF2gcV"
      },
      "outputs": [],
      "source": [
        "df_edge_list_DC_2000_aff = get_edge_dataframe(DC_data_2000, edge_name='Affiliation')\n",
        "adjacency_DC_2000_aff = create_adjacency_matirx(df_edge_list_DC_2000_aff)\n",
        "adjacency_DC_2000_aff = normalize_adjacency_matrix(adjacency_DC_2000_aff)\n",
        "\n",
        "df_edge_list_DC_2000_rel = get_edge_dataframe(DC_data_2000, edge_name='Relatives')\n",
        "adjacency_DC_2000_rel = create_adjacency_matirx(df_edge_list_DC_2000_rel)\n",
        "adjacency_DC_2000_rel = normalize_adjacency_matrix(adjacency_DC_2000_rel)\n",
        "\n",
        "df_edge_list_DC_2000_com = get_edge_dataframe(DC_data_2000, edge_name='Comics')\n",
        "adjacency_DC_2000_com = create_adjacency_matirx(df_edge_list_DC_2000_com)\n",
        "adjacency_DC_2000_com = normalize_adjacency_matrix(adjacency_DC_2000_com)\n",
        "\n",
        "adjacency_DC_2000 = adjacency_DC_2000_aff + 0.5 * adjacency_DC_2000_rel + 0.1 * adjacency_DC_2000_com\n",
        "\n",
        "graph_DC_2000 = nx.from_numpy_array(adjacency_DC_2000)\n",
        "\n",
        "set_nodes_attributes(DC_data_2000, graph_DC_2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8pNIpAn3_Sm"
      },
      "outputs": [],
      "source": [
        "graph_DC_2000.nodes[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualisation_download(adjacency_DC_2000_aff, \"adjacency_DC_2000_aff\", DC_data_2000, gexf_path)\n",
        "visualisation_download(adjacency_DC_2000_rel, \"adjacency_DC_2000_rel\", DC_data_2000, gexf_path)\n",
        "visualisation_download(adjacency_DC_2000_com, \"adjacency_DC_2000_com\", DC_data_2000, gexf_path)\n",
        "visualisation_download(adjacency_DC_2000, \"adjacency_DC_2000\", DC_data_2000, gexf_path)"
      ],
      "metadata": {
        "id": "HmqHYv8kl_8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graphs(\"DC_2000\", images_path)"
      ],
      "metadata": {
        "id": "Ol_8vOwEmK8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a more detailed view of the final graph for DC 2000 you can visit [our interactive page](https://potamitisn.github.io/NML_Project/Networks/DC_2000/)"
      ],
      "metadata": {
        "id": "G3Rk7_Hktxf-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8oqC4f822Qm"
      },
      "source": [
        "**Create network for DC 2001**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtGgA8Qf23-8"
      },
      "outputs": [],
      "source": [
        "df_edge_list_DC_2001_aff = get_edge_dataframe(DC_data_2001, edge_name='Affiliation')\n",
        "adjacency_DC_2001_aff = create_adjacency_matirx(df_edge_list_DC_2001_aff)\n",
        "adjacency_DC_2001_aff = normalize_adjacency_matrix(adjacency_DC_2001_aff)\n",
        "\n",
        "df_edge_list_DC_2001_rel = get_edge_dataframe(DC_data_2001, edge_name='Relatives')\n",
        "adjacency_DC_2001_rel = create_adjacency_matirx(df_edge_list_DC_2001_rel)\n",
        "adjacency_DC_2001_rel = normalize_adjacency_matrix(adjacency_DC_2001_rel)\n",
        "\n",
        "df_edge_list_DC_2001_com = get_edge_dataframe(DC_data_2001, edge_name='Comics')\n",
        "adjacency_DC_2001_com = create_adjacency_matirx(df_edge_list_DC_2001_com)\n",
        "adjacency_DC_2001_com = normalize_adjacency_matrix(adjacency_DC_2001_com)\n",
        "\n",
        "adjacency_DC_2001 = adjacency_DC_2001_aff + 0.5 * adjacency_DC_2001_rel + 0.1 * adjacency_DC_2001_com\n",
        "\n",
        "graph_DC_2001 = nx.from_numpy_array(adjacency_DC_2001)\n",
        "\n",
        "set_nodes_attributes(DC_data_2001, graph_DC_2001)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualisation_download(adjacency_DC_2001_aff, \"adjacency_DC_2001_aff\", DC_data_2001, gexf_path)\n",
        "visualisation_download(adjacency_DC_2001_rel, \"adjacency_DC_2001_rel\", DC_data_2001, gexf_path)\n",
        "visualisation_download(adjacency_DC_2001_com, \"adjacency_DC_2001_com\", DC_data_2001, gexf_path)\n",
        "visualisation_download(adjacency_DC_2001, \"adjacency_DC_2001\", DC_data_2001, gexf_path)"
      ],
      "metadata": {
        "id": "Q_6fPZsom0TK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUVdQuD14C9M"
      },
      "outputs": [],
      "source": [
        "graph_DC_2001.nodes[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graphs(\"DC_2001\", images_path)"
      ],
      "metadata": {
        "id": "C6IdWZXVnEms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a more detailed view of the final graph for DC 2001 you can visit [our interactive page](https://potamitisn.github.io/NML_Project/Networks/DC_2001/)"
      ],
      "metadata": {
        "id": "I89zM0EWt5RZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4k8ijk8mFyK"
      },
      "source": [
        "**Save adjacency matrices**\n",
        "\n",
        "Computing the adjacency matrices takes about 1 minute for each dataset so at this point we can save them. Next time we need them, we can load them directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8yy4dDXLgB2"
      },
      "outputs": [],
      "source": [
        "np.save(os.path.join(graphs_path, \"adjacency_maxtirx_marvel_2000.npy\"), adjacency_mar_2000)\n",
        "np.save(os.path.join(graphs_path, \"adjacency_maxtirx_marvel_2001.npy\"), adjacency_mar_2001)\n",
        "np.save(os.path.join(graphs_path, \"adjacency_maxtirx_DC_2000.npy\"), adjacency_DC_2000)\n",
        "np.save(os.path.join(graphs_path, \"adjacency_maxtirx_DC_2001.npy\"), adjacency_DC_2001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cW0Wi2NIvnt"
      },
      "source": [
        "**Load adjacency matrices and Create Graphs**\n",
        "\n",
        "In case the adjacency matrices are already calculated we can load them directly to speed up the process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1SG3yWyIyam"
      },
      "outputs": [],
      "source": [
        "current_path = os.getcwd()\n",
        "graphs_path = os.path.join(current_path, 'graphs')\n",
        "\n",
        "adjacency_mar_2000 = np.load(os.path.join(graphs_path, \"adjacency_maxtirx_marvel_2000.npy\"))\n",
        "adjacency_mar_2001 = np.load(os.path.join(graphs_path, \"adjacency_maxtirx_marvel_2001.npy\"))\n",
        "adjacency_DC_2000 = np.load(os.path.join(graphs_path, \"adjacency_maxtirx_DC_2000.npy\"))\n",
        "adjacency_DC_2001 = np.load(os.path.join(graphs_path, \"adjacency_maxtirx_DC_2001.npy\"))\n",
        "\n",
        "graph_mar_2000 = nx.from_numpy_array(adjacency_mar_2000)\n",
        "set_nodes_attributes(marvel_data_2000, graph_mar_2000)\n",
        "\n",
        "graph_mar_2001 = nx.from_numpy_array(adjacency_mar_2001)\n",
        "set_nodes_attributes(marvel_data_2001, graph_mar_2001)\n",
        "\n",
        "graph_DC_2000 = nx.from_numpy_array(adjacency_DC_2000)\n",
        "set_nodes_attributes(DC_data_2000, graph_DC_2000)\n",
        "\n",
        "graph_DC_2001 = nx.from_numpy_array(adjacency_DC_2001)\n",
        "set_nodes_attributes(DC_data_2001, graph_DC_2001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTZk207y0qhO"
      },
      "source": [
        "# Exploratory Data Analysis\n",
        "\n",
        "In this section we will perform some EDA to get a precise understanding of the graphs that we are dealing with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfEyLM4FG3bo"
      },
      "source": [
        "## Labels distribution\n",
        "\n",
        "We will begin by taking a look at the number of characters classified as `Good`, `Bad`or `Neutral`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIscqLJFJNpa"
      },
      "outputs": [],
      "source": [
        "def plotLabelDistribution(df, axIndex, name='Marvel 2000'):\n",
        "    \"\"\"\n",
        "    Plots a barplot with the number of good, bad and neutral characters\n",
        "    in our dataset\n",
        "\n",
        "    Inputs \n",
        "      df [pandas.DataFrame] : Dataset dataframe\n",
        "      ax [matplotlib.axes._subplots.AxesSubplot] : Position of the graph\n",
        "      name [str] : Name of the graph to plot. Can be one of the following \n",
        "                   [\"Marvel 2000\", \"Marvel 2001\", \"DC 2000\", \"DC 2001\"]\n",
        "    \"\"\"\n",
        "    labels = df['Good'].value_counts()\n",
        "    labels = labels.rename(index={1: 'Good', 0:'Neutral', -1:'Bad'})\n",
        "    ax = sns.barplot(x=labels.index, y=labels.values, ax=axIndex)\n",
        "    ax.set_title('{} labels distribution'.format(name), fontweight=\"bold\", fontsize=15)\n",
        "    ax.set_ylabel('Number', fontweight=\"bold\", fontsize=15)\n",
        "    ax.set_xlabel('Class', fontweight=\"bold\", fontsize=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25NIC5LCLBsQ"
      },
      "outputs": [],
      "source": [
        "sns.set_theme(style=\"whitegrid\")\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "plotLabelDistribution(marvel_data_2000, axes[0][0], name='Marvel 2000')\n",
        "plotLabelDistribution(marvel_data_2001, axes[0][1], name='Marvel 2001')\n",
        "plotLabelDistribution(DC_data_2000, axes[1][0], name='DC 2000')\n",
        "plotLabelDistribution(DC_data_2000, axes[1][1], name='DC 2001')\n",
        "plt.savefig(os.path.join(figures_path, 'label_distribution_node.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCeqpPLBZEeg"
      },
      "source": [
        "From above figures, we can see the labels distributions of all datasets are **unbalanced**, especially in the DC case. Therefore, to do the node classification task, we can not just consider the accuracy. We should also consider the **F1 score and confusion matrix.**\n",
        "\n",
        "Apart from that, we also need to split the dataset into train set and test set in **stratified fashion**, which makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to the **labels distributions**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFdNuv_fI6ql"
      },
      "source": [
        "## Sparsity pattern of the adjacency matrices\n",
        "\n",
        "Next, we will take a look into the sparsity patter of the final adjacency matrix for each dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfVbat9VI898"
      },
      "outputs": [],
      "source": [
        "# Sparsity for Marvel 2000\n",
        "sort_mar_2000 = np.argsort(adjacency_mar_2000.sum(1))\n",
        "adjacency_mar_2000_sort = adjacency_mar_2000[sort_mar_2000,:][:,sort_mar_2000]\n",
        "\n",
        "# Sparsity for Marvel 2001\n",
        "sort_mar_2001 = np.argsort(adjacency_mar_2001.sum(1))\n",
        "adjacency_mar_2001_sort = adjacency_mar_2001[sort_mar_2001,:][:,sort_mar_2001]\n",
        "\n",
        "# Sparsity for DC 2000\n",
        "sort_DC_2000 = np.argsort(adjacency_DC_2000.sum(1))\n",
        "adjacency_DC_2000_sort = adjacency_DC_2000[sort_DC_2000,:][:,sort_DC_2000]\n",
        "\n",
        "# Sparsity for DC 2001\n",
        "sort_DC_2001 = np.argsort(adjacency_DC_2001.sum(1))\n",
        "adjacency_DC_2001_sort = adjacency_DC_2001[sort_DC_2001,:][:,sort_DC_2001]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting our results."
      ],
      "metadata": {
        "id": "1CqH5RxtvmcH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge2nRWlKJsTF"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(16, 16))\n",
        "axes[0][0].set_title('Marvel 2000 graph: adjacency matrix sparsity pattern', fontweight=\"bold\", fontsize=15)\n",
        "axes[0][0].spy(adjacency_mar_2000_sort)\n",
        "axes[0][1].set_title('Marvel 2001 graph: adjacency matrix sparsity pattern', fontweight=\"bold\", fontsize=15)\n",
        "axes[0][1].spy(adjacency_mar_2001_sort)\n",
        "axes[1][0].set_title('DC 2000 graph: adjacency matrix sparsity pattern', fontweight=\"bold\", fontsize=15)\n",
        "axes[1][0].spy(adjacency_DC_2000_sort)\n",
        "axes[1][1].set_title('DC 2001 graph: adjacency matrix sparsity pattern', fontweight=\"bold\", fontsize=15)\n",
        "axes[1][1].spy(adjacency_DC_2001_sort)\n",
        "\n",
        "plt.savefig(os.path.join(figures_path, 'sparsity_patterns.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac9nkdOCZyRf"
      },
      "source": [
        "From the above figures, we can see the graphs are very sparse. Not all characters are connected. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yLREgeKEdg3"
      },
      "source": [
        "## Degree Distribution and Moments\n",
        "\n",
        "Moving on, we will compute the degree distribution and moments for each graph/adjacency."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Degree distribution"
      ],
      "metadata": {
        "id": "wcsqRP4kxhdf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8ck_gtIEkCP"
      },
      "outputs": [],
      "source": [
        "# Degree for Marvel 2000\n",
        "degree_mar_2000 = adjacency_mar_2000.sum(1)\n",
        "deg_hist_norm_mar_2000 = np.ones(adjacency_mar_2000.shape[0]) / adjacency_mar_2000.sum()\n",
        "\n",
        "# Degree for Marvel 2001\n",
        "degree_mar_2001 = adjacency_mar_2001.sum(1)\n",
        "deg_hist_norm_mar_2001 = np.ones(adjacency_mar_2001.shape[0]) / adjacency_mar_2001.sum()\n",
        "\n",
        "# Degree for DC 2000\n",
        "degree_DC_2000 = adjacency_DC_2000.sum(1)\n",
        "deg_hist_norm_DC_2000 = np.ones(adjacency_DC_2000.shape[0]) / adjacency_DC_2000.sum()\n",
        "\n",
        "# Degree for DC 2001\n",
        "degree_DC_2001 = adjacency_DC_2001.sum(1)\n",
        "deg_hist_norm_DC_2001 = np.ones(adjacency_DC_2001.shape[0]) / adjacency_DC_2001.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting our results."
      ],
      "metadata": {
        "id": "Q5bcz9p5vp7C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKKfjntqFNMv"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "sns.histplot(ax=axes[0][0], x=degree_mar_2000, weights=deg_hist_norm_mar_2000, bins=20, kde=True)\n",
        "axes[0][0].set_title('Marvel 2000 graph degree distribution', fontweight=\"bold\", fontsize=15)\n",
        "axes[0][0].set_xlabel('Degree', fontweight=\"bold\", fontsize=15)\n",
        "axes[0][0].set_ylabel('Normalized count', fontweight=\"bold\", fontsize=15)\n",
        "\n",
        "sns.histplot(ax=axes[0][1], x=degree_mar_2001, weights=deg_hist_norm_mar_2001, bins=20, kde=True)\n",
        "axes[0][1].set_title('Marvel 2001 degree distribution', fontweight=\"bold\", fontsize=15)\n",
        "axes[0][1].set_xlabel('Degree', fontweight=\"bold\", fontsize=15)\n",
        "axes[0][1].set_ylabel('Normalized count', fontweight=\"bold\", fontsize=15)\n",
        "\n",
        "sns.histplot(ax=axes[1][0], x=degree_DC_2000, weights=deg_hist_norm_DC_2000, bins=20, kde=True)\n",
        "axes[1][0].set_title('DC 2000 graph degree distribution', fontweight=\"bold\", fontsize=15)\n",
        "axes[1][0].set_xlabel('Degree', fontweight=\"bold\", fontsize=15)\n",
        "axes[1][0].set_ylabel('Normalized count', fontweight=\"bold\", fontsize=15)\n",
        "\n",
        "sns.histplot(ax=axes[1][1], x=degree_DC_2001, weights=deg_hist_norm_DC_2001, bins=20, kde=True)\n",
        "axes[1][1].set_title('DC 2001 degree distribution', fontweight=\"bold\", fontsize=15)\n",
        "axes[1][1].set_xlabel('Degree', fontweight=\"bold\", fontsize=15)\n",
        "axes[1][1].set_ylabel('Normalized count', fontweight=\"bold\", fontsize=15)\n",
        "\n",
        "plt.savefig(os.path.join(figures_path, 'degree_distributions.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Moments"
      ],
      "metadata": {
        "id": "BSywfUE-xoP5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugXYdWayGEG6"
      },
      "outputs": [],
      "source": [
        "def first_second_moment(A, name='Marvel 2000'):\n",
        "    \"\"\"\n",
        "    Recieves an adjacency matrix for which it computes and then prints\n",
        "    the first and second moment.\n",
        "\n",
        "    Inputs\n",
        "      A [numpy.array] : Adjacency matrix\n",
        "      name [str] : Name of the Adjacency matrix to include at the prit\n",
        "                   (just for the sake of clarity)\n",
        "    \"\"\"\n",
        "    degree = A.sum(1)\n",
        "    values, counts = np.unique(degree, return_counts=True)\n",
        "    \n",
        "    probs = counts / counts.sum()\n",
        "\n",
        "    moment_1 = (values * probs).sum()\n",
        "    moment_2 = (values * values * probs).sum()\n",
        "    \n",
        "    print('First moment for {} is : {:.2f}'.format(name, moment_1))\n",
        "    print('Second moment for {} is : {:.2f}\\n'.format(name, moment_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZClAktAHZFq"
      },
      "outputs": [],
      "source": [
        "first_second_moment(adjacency_mar_2000, name='Marvel 2000')\n",
        "first_second_moment(adjacency_mar_2001, name='Marvel 2001')\n",
        "first_second_moment(adjacency_DC_2000, name='DC 2000')\n",
        "first_second_moment(adjacency_DC_2001, name='DC 2001')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwF0O_vvaUBP"
      },
      "source": [
        "**Comment:**\n",
        "\n",
        "From above figures, we can see all the degree distributions are based on power law (or fat-tailed distribution). This is the **social network's behaviour**.\n",
        "\n",
        "Apart from that, we notice that the second moment, $\\left\\langle k^{2}\\right\\rangle$ is large. We also notice there are some hubs (high-degree nodes). Therefore, this is the scale-free behaviour."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb2JZq50H8r0"
      },
      "source": [
        "### Total degree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C59Mjqf6IAQ8"
      },
      "outputs": [],
      "source": [
        "def total_degree_print(A, name='Marvel 2000'):\n",
        "    edges = A.sum()\n",
        "    print('Total degree for {} is : {:.1f}\\n'.format(name, edges))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvJZHAyzITPN"
      },
      "outputs": [],
      "source": [
        "total_degree_print(adjacency_mar_2000, name='Marvel 2000')\n",
        "total_degree_print(adjacency_mar_2001, name='Marvel 2001')\n",
        "total_degree_print(adjacency_DC_2000, name='DC 2000')\n",
        "total_degree_print(adjacency_DC_2001, name='DC 2001')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiIQEUkXbYgT"
      },
      "source": [
        "From above printings, we can see Marvel graphs have more edges than those of DC datasets. Therefore, more characters in Marvel universe are connected. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP6-9LGmf9y1"
      },
      "source": [
        "## Important Nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCavg4k-ceoV"
      },
      "source": [
        "To verify our network creation is meaningful, we visulaize the top 10 important nodes based on **Degree** and **Betweenness Centrality.** The details of the two global properties are shown in the following:\n",
        "\n",
        "**Degree:** In this project, this is simply the normalized number of connections the node has in the network. In the Marvel and DC universe, this corresponds to the total number of common affiliations, relatives and comics which the two characters have. For example, imagine if you are Iron man, you will be very important because you will have a lot of common affiliations and relatives with other characters. You also appear in a lot of comics.\n",
        "\n",
        "**Betweenness Centrality:** this corresponds to how many shortest paths in the network lead through the node. For example, imagine you are Iron man and you want to send a message to Wolverine. The shortest path how to send it is via Spider man, because he interacted both with Iron man and Wolverine. On the other side, if you want to send a message to Captain America, you dont have to go through Spider man because Iron man knows Captain America directly. The betweenness centrality for Spider man is computed using the number of shortest paths between all other characters that pass through him."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THvayeFzgBXk"
      },
      "outputs": [],
      "source": [
        "def plotNodeDegree(graph, axIndex, name='Marvel 2000'):\n",
        "    \"\"\"\n",
        "    Plots top 10 most importand nodes of a given graph based on degree \n",
        "\n",
        "    Inputs\n",
        "      graph [networkx.classes.graph.Graph] : Graph to be inspected\n",
        "      axIndex [matplotlib.axes._subplots.AxesSubplot] : Position of the graph\n",
        "      name [str] : Name of the graph to plot (just used for the title). Should be\n",
        "                   one of the following : [\"Marvel 2000\", \"Marvel 2001\", \"DC 2000\", \"DC 2001\"]\n",
        "    \"\"\"\n",
        "    topNode = sorted(graph.degree(weight='weight'), key=lambda x: x[1], reverse=True)[:10]\n",
        "    deg = [node[1] for node in topNode]\n",
        "    alias = [graph.nodes[node[0]]['Current Alias'] if graph.nodes[node[0]]['Current Alias'] != '' else graph.nodes[node[0]]['Real Name'] for node in topNode]\n",
        "\n",
        "    ax = sns.barplot(x=deg, y=alias, ax=axIndex, ci=None)\n",
        "    ax.set_title('{} Top 10 Node Degree'.format(name), fontweight=\"bold\", fontsize=15)\n",
        "    ax.set_ylabel('Current Alias', fontweight=\"bold\", fontsize=15)\n",
        "    ax.set_xlabel('Degree', fontweight=\"bold\", fontsize=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOsphC29jcEQ"
      },
      "outputs": [],
      "source": [
        "def plotNodeBetween(graph, axIndex, name='Marvel 2000'):\n",
        "    \"\"\"\n",
        "    Plots top 10 most importand nodes of a given graph based on betwenness centrality \n",
        "\n",
        "    Inputs\n",
        "      graph [networkx.classes.graph.Graph] : Graph to be inspected\n",
        "      axIndex [matplotlib.axes._subplots.AxesSubplot] : Position of the graph\n",
        "      name [str] : Name of the graph to plot (just used for the title). Should be\n",
        "                   one of the following : [\"Marvel 2000\", \"Marvel 2001\", \"DC 2000\", \"DC 2001\"]\n",
        "    \"\"\"\n",
        "    topNode = sorted(nx.betweenness_centrality(graph, weight=None).items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "    betweenness_centrality = [node[1] for node in topNode]\n",
        "    alias = [graph.nodes[node[0]]['Current Alias'] if graph.nodes[node[0]]['Current Alias'] != '' else graph.nodes[node[0]]['Real Name'] for node in topNode]\n",
        "\n",
        "    ax = sns.barplot(x=betweenness_centrality, y=alias, ax=axIndex, ci=None)\n",
        "    ax.set_title('{} Top 10 Node Betweenness Centrality'.format(name), fontweight=\"bold\", fontsize=15)\n",
        "    ax.set_ylabel('Current Alias', fontweight=\"bold\", fontsize=15)\n",
        "    ax.set_xlabel('Degree', fontweight=\"bold\", fontsize=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top 10 Important nodes based on Degree\n"
      ],
      "metadata": {
        "id": "I4Zbui5n1Pys"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1r8jNk7iL7H"
      },
      "outputs": [],
      "source": [
        "sns.set_theme(style=\"whitegrid\", font_scale=1.5)\n",
        "fig, axes = plt.subplots(2, 2, figsize=(39, 15))\n",
        "plotNodeDegree(graph_mar_2000, axes[0][0], name='Marvel 2000')\n",
        "plotNodeDegree(graph_mar_2001, axes[0][1], name='Marvel 2001')\n",
        "plotNodeDegree(graph_DC_2000, axes[1][0], name='DC 2000')\n",
        "plotNodeDegree(graph_DC_2001, axes[1][1], name='DC 2001')\n",
        "\n",
        "plt.savefig(os.path.join(figures_path, 'top_nodes_degree.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top 10 importand nodes based on betwenness centrality"
      ],
      "metadata": {
        "id": "eYEI7UEm1bzN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BObDRomukO6O"
      },
      "outputs": [],
      "source": [
        "sns.set_theme(style=\"whitegrid\", font_scale=1.5)\n",
        "fig, axes = plt.subplots(2, 2, figsize=(43, 15))\n",
        "plotNodeBetween(graph_mar_2000, axes[0][0], name='Marvel 2000')\n",
        "plotNodeBetween(graph_mar_2001, axes[0][1], name='Marvel 2001')\n",
        "plotNodeBetween(graph_DC_2000, axes[1][0], name='DC 2000')\n",
        "plotNodeBetween(graph_DC_2001, axes[1][1], name='DC 2001')\n",
        "\n",
        "plt.savefig(os.path.join(figures_path, 'top_nodes_betweenness.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhPNr4R7gcP_"
      },
      "source": [
        "From above figures, we can see all important characters meet our expectations. For example, Iron man and Spider man are very popular in Marvel universe. Also, Superman and Batman are very popular in DC universe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67Y5bRVCRz6z"
      },
      "source": [
        "## Path matrix (N = 10)\n",
        "\n",
        "Moving forward we will compute the path matrix as a mean of measurement concerning the connectivity of the graphs. The path matrix is defined by :\n",
        "\n",
        "$$\n",
        " P_{ij} = \\displaystyle\\sum_{k=0}^{N}C_{k}(i,j). \n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzdOJYrMR2jL"
      },
      "outputs": [],
      "source": [
        "def path_matrix(A, n):\n",
        "    \"\"\"\n",
        "    Given an adjacency matrix, it computes and returns its path matrix for N=n\n",
        "\n",
        "    Inputs :\n",
        "      A [numpy.array] : Adjacency matrix\n",
        "      n [int] : Power of up to which the path matrix is computed. Corresponds to\n",
        "                N from the above formula\n",
        "    Outputs\n",
        "      path_matrix [numpy.array] : Computed path matrix\n",
        "    \"\"\"\n",
        "    path_lengths = range(11)\n",
        "    path_matrix = sum([np.linalg.matrix_power(A, k) for k in path_lengths])\n",
        "\n",
        "    return path_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6tvBUTrSmdT"
      },
      "outputs": [],
      "source": [
        "# Computing path matrix for Marvel 2000\n",
        "path_mar_2000 = path_matrix(adjacency_mar_2000, n=10)\n",
        "\n",
        "#Computing path matrix for Marvel 2001\n",
        "path_mar_2001 = path_matrix(adjacency_mar_2001, n=10)\n",
        "\n",
        "#Computing path matrix for DC 2000\n",
        "path_DC_2000 = path_matrix(adjacency_DC_2000, n=10)\n",
        "\n",
        "#Computing path matrix for DC 2001\n",
        "path_DC_2001 = path_matrix(adjacency_DC_2001, n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting our results :"
      ],
      "metadata": {
        "id": "nEseEBvL36Jq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esRUpwciS3vs"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(16, 16))\n",
        "axes[0][0].set_title('Marvel 2000 graph: path matrix sparsity pattern', fontweight=\"bold\", fontsize=15)\n",
        "axes[0][0].spy(path_mar_2000)\n",
        "axes[0][1].set_title('Marvel 2001 graph: path matrix sparsity pattern', fontweight=\"bold\", fontsize=15)\n",
        "axes[0][1].spy(path_mar_2001)\n",
        "axes[1][0].set_title('DC 2000 graph: path matrix sparsity pattern', fontweight=\"bold\", fontsize=15)\n",
        "axes[1][0].spy(path_DC_2000)\n",
        "axes[1][1].set_title('DC 2001 graph: path matrix sparsity pattern', fontweight=\"bold\", fontsize=15)\n",
        "axes[1][1].spy(path_DC_2001)\n",
        "\n",
        "plt.savefig(os.path.join(figures_path, 'path_matrix.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGBe2_DOgvvn"
      },
      "source": [
        "From above figures, we can see once more that the Marvel graphs are more connected than DC graphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-9lofEnbZUK"
      },
      "source": [
        "## Connected compoents, giant components and clustering coefficients\n",
        "\n",
        "In this section we will study the connected componets, the giant component and the average clustering coefficient of the graphs. The average clustering coefficient is given by : \n",
        "\n",
        "$$\n",
        "C = \\frac{1}{N}_{i=1}^NC_i \n",
        "$$\n",
        "\n",
        "where $C_i$ is the clustering coefficient of each node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PueffWoqcGJK"
      },
      "outputs": [],
      "source": [
        "def print_connected_giant_clustering(graph, name='Marvel 2000'):\n",
        "    \"\"\"\n",
        "    Given a graph it prints some statements concering the connected components\n",
        "    of the graph and its average clustering coefficient\n",
        "\n",
        "    Inputs \n",
        "      graph [networkx.classes.graph.Graph] : Graph to be inspected\n",
        "      name [str] : Name of the graph to be included in the print (just for clarity)\n",
        "    \"\"\"\n",
        "    giant_com = graph.subgraph(max(nx.connected_components(graph), key=len)).copy()\n",
        "    n_connected_com = nx.number_connected_components(graph)\n",
        "    average_clustering_coe = nx.average_clustering(graph)\n",
        "    \n",
        "    print('{} has {} nodes and {:.1f} edges/total weight'.format(name, graph.number_of_nodes(), graph.size(weight='weight')))\n",
        "    print('{} number of connected components: {}'.format(name, n_connected_com))\n",
        "    print('The giant component of {} with diameter {} has {} nodes and {:.1f} edges/total weight.'.format(name, nx.diameter(giant_com), giant_com.number_of_nodes(), giant_com.size(weight='weight')))\n",
        "    print('{} average clustering coefficient: {:.3f}\\n'.format(name, average_clustering_coe))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmjbw5t8dINw"
      },
      "outputs": [],
      "source": [
        "print_connected_giant_clustering(graph_mar_2000, name='Marvel 2000')\n",
        "print_connected_giant_clustering(graph_mar_2001, name='Marvel 2001')\n",
        "print_connected_giant_clustering(graph_DC_2000, name='DC 2000')\n",
        "print_connected_giant_clustering(graph_DC_2001, name='DC 2001')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtJ36t1Og5DW"
      },
      "source": [
        "From above printings, we can see DC universe has more connected components that Marvel universe. Therefore, DC universe does not want to connect all characters. DC universe probably would like to make stories about some characters not all characters. \n",
        "\n",
        "We can also see Marvel universe and DC universe have the similar average clustering coefficients. Clustering coefficient can show the connectivity pattern of a node to its neighbors. Relatively large values are observed for the networks having more regular connectivity patterns. From above printings, we find the clustering coeficients of the Marvel and DC graphs are large. Therefore, the two graphs are not random networks. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M6IvLFFNYxG"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISBmW0nH2eQe"
      },
      "source": [
        "For feature extraction, we consider following five types of features:\n",
        "\n",
        "**Hand-crafted features:**\n",
        "*   Degree\n",
        "*   Betweenness centrality\n",
        "*   Closeness centrality\n",
        "*   Eigenvector centrality\n",
        "*   Clustering coefficients\n",
        "\n",
        "We choose above five hand-crafted features for the following reasons : \n",
        "\n",
        "- Firstly, based on the exploratory data analysis, we see node degree and betweenness centrality provide meaningful information about the important nodes (please see the discussion in Exploratory Data Analysis). \n",
        "\n",
        "- Secondly, closeness centrality measures the importance of a node based on how many shortest path lengths to all other nodes. For example, Iron man can interact with other **good** characters directly. Iron man can have common affiliations and relatives with good characters such as Spider man and Captain America. Eigenvector centrality corresponds to a node surrounded by how many important neighbors. For example, Mentor is a good character in Marvel, but he is not very popular. Mentor can be surrounded by important good characters.\n",
        "\n",
        "- Thirdly, we use clustering coefficients to provide information about the subgraph containing the neighbors of a node, and all edges between nodes in its neighborhood. This can measure about the connectivity pattern of a node to its neighbors and provide information about the network model.\n",
        "\n",
        "**More flexible node representations:**\n",
        "\n",
        "**Node2vec** to provide 30 dimensional feature vector for each node.\n",
        "\n",
        "We use Node2vec (with $p = 1.0$ and $q = 10.0$) to characterize the local view of the network. We notice in social-network behavioural network, the communities of nodes are not very clear. Therefore, we use Node2vec (with $p = 1.0$ and $q = 10.0$) for capturing structural nodes, e.g., hubs, outliers.\n",
        "\n",
        "**Labels**\n",
        "For labels of nodes, we use the `Good` feature of nodes (Integer value representing whether the character is good (+1), neutral (0), or evil (-1)). We add one to the interger values, so the character is good (+2), neutral (1), or evil (0). This can help us train graph nerual networks with cross-entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJVo6iBoPvOC"
      },
      "outputs": [],
      "source": [
        "def Node2Vec(G, dimensions, walk_length, num_walks, p=1.0, q=1.0):\n",
        "    \"\"\"\n",
        "    Creates additional dimeansional feature vector for each vector\n",
        "\n",
        "    Inputs\n",
        "      G [networkx.classes.graph.Graph] : Graph to be inspected\n",
        "      dimensions [int]: Embedding dimensions\n",
        "      walk_length [int]: Maximum length of each random walk\n",
        "      num_walks [int]: Total number of random walks per root node\n",
        "      p [float]:\n",
        "      q [float]: \n",
        "    \"\"\"\n",
        "    seed = 0\n",
        "    rw = BiasedRandomWalk(G)\n",
        "    walks = rw.run(\n",
        "        nodes=G.nodes(),\n",
        "        length=walk_length,\n",
        "        n=num_walks,\n",
        "        p=p,\n",
        "        q=q,\n",
        "        seed=seed\n",
        "    )\n",
        "    str_walks = [[str(n) for n in walk] for walk in walks]\n",
        "    model = Word2Vec(str_walks, size=dimensions, window=5, min_count=0, sg=1, seed=seed, workers=1)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Gu7fuZ5Na7v"
      },
      "outputs": [],
      "source": [
        "def extract_features(G, nodewalk_dim=30):\n",
        "  \"\"\"\n",
        "\n",
        "  Inputs\n",
        "    G [networkx.classes.graph.Graph] : Graph to be inspected\n",
        "    nodewalk_dim [int]: Embedding dimensions for Node2Vec function\n",
        "  \n",
        "  Outputs\n",
        "    Features [numpy.array] : NxD where N is the number of nodes and D(=35) are the\n",
        "                             features created by this function \n",
        "  \"\"\"\n",
        "  ## degree\n",
        "  degrees = G.degree(weight='weight')\n",
        "  degree_feature = np.array([degrees[node] for node in G.nodes()])\n",
        "  \n",
        "  ## 3 node centrality measures\n",
        "  betweenness_centrality = nx.betweenness_centrality(G, weight=None)\n",
        "  closeness_centrality = nx.closeness_centrality(G, distance=None)\n",
        "  eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=500, weight=None)\n",
        "  betweenness_centrality_feature = np.array([betweenness_centrality[node] for node in G.nodes()])\n",
        "  closeness_centrality_feature = np.array([closeness_centrality[node] for node in G.nodes()])\n",
        "  eigenvector_centrality_feature = np.array([eigenvector_centrality[node] for node in G.nodes()])\n",
        "\n",
        "  ## clustering coefficient of each node\n",
        "  clustering_coefficients = nx.clustering(G, weight=None)\n",
        "  clustering_coefficient_feature = np.array([clustering_coefficients[node] for node in G.nodes()])\n",
        "\n",
        "  ## Node2walk\n",
        "  model = Node2Vec(StellarGraph.from_networkx(G, edge_weight_attr=None), dimensions=nodewalk_dim, walk_length=10, num_walks=50, p=1.0, q=10.0)\n",
        "  features_n2v1 = np.array([model.wv.get_vector(str(node)) for node in G.nodes()])\n",
        "  \n",
        "  ## stack the features\n",
        "  Features = np.concatenate((degree_feature[:, np.newaxis], betweenness_centrality_feature[:, np.newaxis], \n",
        "                             closeness_centrality_feature[:, np.newaxis], eigenvector_centrality_feature[:, np.newaxis], clustering_coefficient_feature[:, np.newaxis], features_n2v1), axis=1)\n",
        "\n",
        "  return Features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting the features\n",
        "\n",
        "In the following code blocs we will extract the features described so far. For each dataset it takes about 3 minutes so for time reasons we saved them already. If you want you can skip the next 5 code blocks and load them direclty with the provided code."
      ],
      "metadata": {
        "id": "6k1ripPiAvxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract features for Marvel 2000 dataset**"
      ],
      "metadata": {
        "id": "TRefnjmg_rX0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be_mvmQGNln5"
      },
      "outputs": [],
      "source": [
        "features_mar_2000 = extract_features(graph_mar_2000)\n",
        "targets_mar_2000 = np.array([nx.get_node_attributes(graph_mar_2000, \"Good\")[node]+1 for node in graph_mar_2000.nodes()])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract features for Marvel 2001 dataset**"
      ],
      "metadata": {
        "id": "CtVaVgr4_xRa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjfKomiCiJBa"
      },
      "outputs": [],
      "source": [
        "features_mar_2001 = extract_features(graph_mar_2001)\n",
        "targets_mar_2001 = np.array([nx.get_node_attributes(graph_mar_2001, \"Good\")[node]+1 for node in graph_mar_2001.nodes()])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract features for DC 2000 dataset**"
      ],
      "metadata": {
        "id": "Hr2oui6s_50k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUP2-l2SkYEE"
      },
      "outputs": [],
      "source": [
        "features_DC_2000 = extract_features(graph_DC_2000)\n",
        "targets_DC_2000 = np.array([nx.get_node_attributes(graph_DC_2000, \"Good\")[node]+1 for node in graph_DC_2000.nodes()])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract features for 2001 dataset**"
      ],
      "metadata": {
        "id": "a-DpOyIc_-Hn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uaw1jl4ykECh"
      },
      "outputs": [],
      "source": [
        "features_DC_2001 = extract_features(graph_DC_2001)\n",
        "targets_DC_2001 = np.array([nx.get_node_attributes(graph_DC_2001, \"Good\")[node]+1 for node in graph_DC_2001.nodes()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAJc7toYlQuq"
      },
      "source": [
        "### Save/Load features and labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the feature and label arrays takes about 3 minutes for each dataset. For this reason we save them to speed up any case where we need them in the future."
      ],
      "metadata": {
        "id": "0n3ktIwlAHVx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXBFvVKqkEEp"
      },
      "outputs": [],
      "source": [
        "np.save(os.path.join(features_path, \"features_marvel_2000.npy\"), features_mar_2000)\n",
        "np.save(os.path.join(features_path, \"y_marvel_2000.npy\"), targets_mar_2000)\n",
        "\n",
        "np.save(os.path.join(features_path, \"features_marvel_2001.npy\"), features_mar_2001)\n",
        "np.save(os.path.join(features_path, \"y_marvel_2001.npy\"), targets_mar_2001)\n",
        "\n",
        "np.save(os.path.join(features_path, \"features_DC_2000.npy\"), features_DC_2000)\n",
        "np.save(os.path.join(features_path, \"y_DC_2000.npy\"), targets_DC_2000)\n",
        "\n",
        "np.save(os.path.join(features_path, \"features_DC_2001.npy\"), features_DC_2001)\n",
        "np.save(os.path.join(features_path, \"y_DC_2001.npy\"), targets_DC_2001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zhpYguQ5iBy"
      },
      "source": [
        "**Load features and labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPezumbPefSP"
      },
      "outputs": [],
      "source": [
        "features_mar_2000 = np.load(os.path.join(features_path, \"features_marvel_2000.npy\"))\n",
        "targets_mar_2000 = np.load(os.path.join(features_path, \"y_marvel_2000.npy\"))\n",
        "\n",
        "features_mar_2001 = np.load(os.path.join(features_path, \"features_marvel_2001.npy\"))\n",
        "targets_mar_2001 = np.load(os.path.join(features_path, \"y_marvel_2001.npy\"))\n",
        "\n",
        "features_DC_2000 = np.load(os.path.join(features_path, \"features_DC_2000.npy\")) \n",
        "targets_DC_2000 = np.load(os.path.join(features_path, \"y_DC_2000.npy\"))\n",
        "\n",
        "features_DC_2001 = np.load(os.path.join(features_path, \"features_DC_2001.npy\"))\n",
        "targets_DC_2001 = np.load(os.path.join(features_path, \"y_DC_2001.npy\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIUkxTBSHyVI"
      },
      "source": [
        "## Spilt dataset in train set and test set, Normalize data\n",
        "\n",
        "In order to be able to evaluate our models we first need to separate our dataset in train dataset (used for training) and test dataset (not used for training). The way to do this network machine learning is to create mask arrays. These arrays have the same dimensions for our feature matrix and label matrix but contain only `True`and `False` values indicating which elements are gonna be used for training and which for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLltkwqADe87"
      },
      "source": [
        "**Train and Test split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42BxxPtMDhEO"
      },
      "outputs": [],
      "source": [
        "def split_train_test(features, targets, ratio=0.2, seed=0):\n",
        "    \"\"\"\n",
        "    Given the train matrix and the label vector computes the masks to be used in \n",
        "    order to spli the dataset into a train set and training set. An important aspect\n",
        "    is that we use the stratify option from the train_test_split function. This\n",
        "    helps us have a balanced training set.\n",
        "\n",
        "    Inputs\n",
        "      features [numpy.ndarray]: NxD Train matrix where N is the number of nodes and \n",
        "                              D the number of features\n",
        "      targets [numpy.ndarray]: Nx1 Label vect where each node is labeld with 0,1 or 2\n",
        "                             depending wether they are good, bad or neutral.\n",
        "      ratio [float]: Portion of dataset to be used just for testing and not training\n",
        "      seed [int]: Random seed\n",
        "\n",
        "    Outputs\n",
        "      train_mask [numpy.ndarray]: NxD Train mask filled with True/False indicating\n",
        "                                  which elements are to be used for training or not\n",
        "      test_mask [numpy.ndarray]: Nx1 Test mask filled with True/False indicating\n",
        "                                  which elements are to be used for testing or not\n",
        "    \"\"\"\n",
        "    train_ratio = ratio\n",
        "    n_nodes = features.shape[0]\n",
        "    n_train = int(n_nodes * train_ratio)\n",
        "    idx = np.array([i for i in range(n_nodes)])\n",
        "    \n",
        "    n_train, n_test, y_train, y_test = train_test_split(idx, targets, stratify=targets, test_size=ratio, random_state = seed)\n",
        "    train_mask = np.full_like(targets, False, dtype=bool)\n",
        "    train_mask[idx[n_train]] = True\n",
        "    test_mask = np.full_like(targets, False, dtype=bool)\n",
        "    test_mask[idx[n_test]] = True\n",
        "\n",
        "    train_mask = train_mask\n",
        "    test_mask = test_mask\n",
        "\n",
        "    return train_mask, test_mask "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrPUBqj-G1Ig"
      },
      "outputs": [],
      "source": [
        "# Creating the masks for Marvel 2000 dataset\n",
        "train_mask_mar_2000, test_mask_mar_2000 = split_train_test(features_mar_2000, targets_mar_2000, ratio=0.2, seed=0)\n",
        "\n",
        "# Creating the masks for Marvel 2001 dataset\n",
        "train_mask_mar_2001, test_mask_mar_2001 = split_train_test(features_mar_2001, targets_mar_2001, ratio=0.2, seed=0)\n",
        "\n",
        "# Creating the masks for DC 2000 dataset\n",
        "train_mask_DC_2000, test_mask_DC_2000 = split_train_test(features_DC_2000, targets_DC_2000, ratio=0.2, seed=0)\n",
        "\n",
        "# Creating the masks for DC 2001 dataset\n",
        "train_mask_DC_2001, test_mask_DC_2001 = split_train_test(features_DC_2001, targets_DC_2001, ratio=0.2, seed=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlSsOoKZIwMP"
      },
      "source": [
        "**Normalize data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJWGftILIyjv"
      },
      "outputs": [],
      "source": [
        "def normalize(features, train_mask, test_mask):\n",
        "    \"\"\"\n",
        "    Normalizes the features\n",
        "\n",
        "    Inputs\n",
        "      features [numpy.ndarray]: NxD Train matrix where N is the number of nodes and \n",
        "                              D the number of features\n",
        "      train_mask [numpy.ndarray]: NxD Train mask filled with True/False indicating\n",
        "                                  which elements are to be used for training or not\n",
        "      test_mask [numpy.ndarray]: Nx1 Test mask filled with True/False indicating\n",
        "                                  which elements are to be used for testing or not\n",
        "    Outputs\n",
        "      normalized_features [numpy.ndarray] : NXD Normalized features matrix\n",
        "    \"\"\"\n",
        "    normalized_features = features.copy()\n",
        "    X_train, X_test = features[train_mask, :].copy(), features[test_mask, :].copy()\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "    X_train = scaler.transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    normalized_features[train_mask, :] = X_train\n",
        "    normalized_features[test_mask, :] = X_test\n",
        "    return normalized_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWCqy8yEJXMv"
      },
      "outputs": [],
      "source": [
        "features_mar_2000 = normalize(features_mar_2000, train_mask_mar_2000, test_mask_mar_2000)\n",
        "features_mar_2001 = normalize(features_mar_2001, train_mask_mar_2001, test_mask_mar_2001)\n",
        "features_DC_2000 = normalize(features_DC_2000, train_mask_DC_2000, test_mask_DC_2000)\n",
        "features_DC_2001 = normalize(features_DC_2001, train_mask_DC_2001, test_mask_DC_2001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWBSVm5ofHWg"
      },
      "source": [
        "**Set features as the attributes of nodes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iQLbDEvfL6I"
      },
      "outputs": [],
      "source": [
        "def set_features_attr(features, labels, train_mask, test_mask, graph):\n",
        "    \"\"\"\n",
        "    Updates the node attributes of the grap with the calculated features\n",
        "\n",
        "    Inputs\n",
        "      features [numpy.ndarray]: NxD Train matrix where N is the number of nodes and \n",
        "                                D the number of features\n",
        "      labels [numpy.ndarray]: Nx1 Label vect where each node is labeld with 0,1 or 2\n",
        "                              depending wether they are good, bad or neutral.\n",
        "      train_mask [numpy.ndarray]: NxD Train mask filled with True/False indicating\n",
        "                                  which elements are to be used for training or not\n",
        "      test_mask [numpy.ndarray]: Nx1 Test mask filled with True/False indicating\n",
        "                                 which elements are to be used for testing or not\n",
        "      graph [networkx.classes.graph.Graph] : Graph for which we want to update the\n",
        "                                             attributes\n",
        "\n",
        "    \"\"\"\n",
        "    x_attr = {i:features[i, :] for i in range(features.shape[0])}\n",
        "    y_attr = {i:labels[i] for i in range(labels.shape[0])}\n",
        "    train_mask_attr = {i:train_mask[i] for i in range(train_mask.shape[0])}\n",
        "    test_mask_attr = {i:test_mask[i] for i in range(test_mask.shape[0])}\n",
        "\n",
        "    nx.set_node_attributes(graph, x_attr, \"x\")\n",
        "    nx.set_node_attributes(graph, y_attr, \"y\")\n",
        "    nx.set_node_attributes(graph, train_mask_attr, \"train_mask\")\n",
        "    nx.set_node_attributes(graph, test_mask_attr, \"test_mask\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okMx6bUchGFh"
      },
      "outputs": [],
      "source": [
        "set_features_attr(features_mar_2000, targets_mar_2000, train_mask_mar_2000, test_mask_mar_2000, graph_mar_2000)\n",
        "set_features_attr(features_mar_2001, targets_mar_2001, train_mask_mar_2001, test_mask_mar_2001, graph_mar_2001)\n",
        "set_features_attr(features_DC_2000, targets_DC_2000, train_mask_DC_2000, test_mask_DC_2000, graph_DC_2000)\n",
        "set_features_attr(features_DC_2001, targets_DC_2001, train_mask_DC_2001, test_mask_DC_2001, graph_DC_2001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qx2pgNXRHLMM"
      },
      "outputs": [],
      "source": [
        "graph_mar_2000.nodes[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjuUSmszrv3X"
      },
      "source": [
        "## Features Visualization (PCA, Isomap, TSNE)\n",
        "\n",
        "At this moment we are dealing with high dimensional data (35 features) which means that we can't visualise them in their current form. To do that we will first have to apply some statistical methods. In this section we will use the following methods to visualise our data : \n",
        "\n",
        "- Principal Components Analysis (PCA)\n",
        "-  Isomap\n",
        "- T-distributed Stochastic Neighbor embedding (TSNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7XfjCwnWDyf"
      },
      "outputs": [],
      "source": [
        "def visualize_features(features, labels, dim=3):\n",
        "    \"\"\"\n",
        "    Inputs\n",
        "      features [numpy.ndarray]: NxD Train matrix where N is the number of nodes and \n",
        "                                D the number of features\n",
        "      labels [numpy.ndarray]: Nx1 Label vect where each node is labeld with 0,1 or 2\n",
        "                              depending wether they are good, bad or neutral.\n",
        "    \"\"\"\n",
        "    pca = PCA(n_components=dim)\n",
        "    tsne = TSNE(n_components=dim)\n",
        "    isomap = Isomap(n_components=dim,n_neighbors=20)\n",
        "    \n",
        "    colors = [\"navy\", \"turquoise\", \"darkorange\"]\n",
        "\n",
        "    pca_features = pca.fit_transform(features)\n",
        "    tsne_features = tsne.fit_transform(features)\n",
        "    isomap_features = isomap.fit_transform(features)\n",
        "    \n",
        "    fig = plt.figure(figsize=(18, 7))\n",
        "   \n",
        "    index = 1\n",
        "    for data, name in zip([pca_features, tsne_features, isomap_features], ['PCA', 'TSNE', 'Isomap']):\n",
        "        ax = fig.add_subplot(1, 3, index, projection='3d')\n",
        "        for color, i, target_name in zip(colors, [0, 1, 2], ['Bad', 'Neutral', 'Good']):\n",
        "            ax.scatter(\n",
        "                data[labels == i, 0], data[labels == i, 1], data[labels == i, 2], color=color, alpha=0.8, lw=2, label=target_name\n",
        "              )\n",
        "            ax.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
        "            ax.set_xlabel(\"x1\", fontweight='bold')\n",
        "            ax.set_ylabel(\"x2\", fontweight='bold')\n",
        "            ax.set_zlabel(\"x3\", fontweight='bold')\n",
        "            ax.set_title(name, fontsize=16, fontweight='bold', loc='left')\n",
        "        index += 1 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9PHHrMokDGi"
      },
      "source": [
        "**Visualising Marvel 2000 feature dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaDgsDa-YMPq"
      },
      "outputs": [],
      "source": [
        "visualize_features(features_mar_2000, targets_mar_2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQdvtZlZkIOd"
      },
      "source": [
        "**Visualising Marvel 2001 feature dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSpUgybBkKar"
      },
      "outputs": [],
      "source": [
        "visualize_features(features_mar_2001, targets_mar_2001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36bVoMOkkUUG"
      },
      "source": [
        "**Visualising DC 2000 feature dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpbhOFVnkWDT"
      },
      "outputs": [],
      "source": [
        "visualize_features(features_DC_2000, targets_DC_2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8jvseQ8kOEM"
      },
      "source": [
        "**Visualising DC 2001 feature dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_hlpQQHkNYD"
      },
      "outputs": [],
      "source": [
        "visualize_features(features_DC_2001, targets_DC_2001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_md2OsvCV6lV"
      },
      "source": [
        "## Graph signal processing and graph Tikhonov regularization\n",
        "\n",
        "To prevent machine learning models from overfitting on the training set, we do the graph Tikhonov regularization in this part. We also focus on graph Fourier transform and graph signal processing."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility functions \n",
        "\n",
        "To achieve this we define a few more utility functions in the following code block :"
      ],
      "metadata": {
        "id": "VrPJmTKnT_3L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2nVaTuUM4ne"
      },
      "outputs": [],
      "source": [
        "def compute_laplacian(adjacency: np.ndarray, normalize: str):\n",
        "    \"\"\"\n",
        "    Computes Laplacian matrix given an adjacency matrix and a normalization method\n",
        "    \n",
        "    Inputs\n",
        "      adjancency [numpy.ndarray]: Adjacency matrix\n",
        "      normalize [str]: can be None, 'sym' or 'rw' for the combinatorial, symmetric normalized or random walk Laplacians\n",
        "    Outputs:\n",
        "        L [numpy.ndarray]: Combinatorial or symmetric normalized Laplacian.\n",
        "    \"\"\"\n",
        "    n = adjacency.shape[0]\n",
        "    D = np.diag(np.sum(adjacency, axis=1))\n",
        "    cm_L = D - adjacency\n",
        "\n",
        "    if normalize == 'sym':\n",
        "        D_sqrt = np.diag(np.clip(np.sum(adjacency, axis=1), 1, None)**(-1/2))\n",
        "        L = D_sqrt @ cm_L @ D_sqrt\n",
        "    elif normalize == 'rw':\n",
        "        L = np.diag(np.clip(np.sum(adjacency, axis=1), 1, None)**(-1)) @ cm_L\n",
        "    else:\n",
        "        L = cm_L\n",
        "\n",
        "    return L\n",
        "\n",
        "def spectral_decomposition(laplacian: np.ndarray):\n",
        "    \"\"\" \n",
        "    Given a Laplacian matrix computes the corresponding eigenvalues and eigenvectors\n",
        "\n",
        "    Inputs\n",
        "      laplacian [numpy.ndarray]: Laplacian matrix\n",
        "    Outputs:\n",
        "      lamb [numpy.ndarray]: Eigenvalues of the Laplacian\n",
        "      U [numpy.ndarray]: Corresponding eigenvectors.\n",
        "    \"\"\"\n",
        "    if np.all(laplacian == laplacian.T):\n",
        "        return np.linalg.eigh(laplacian)\n",
        "    else:\n",
        "        return np.linalg.eig(laplacian) \n",
        "\n",
        "def GFT(signal: np.array, U: np.ndarray):\n",
        "    \"\"\"\n",
        "    Function to compute the graph fourier transform \n",
        "      signal [numpy.ndarray]: float array of size n.\n",
        "      U [numpy.ndarray]: matrix of size n x n containing one eigenvector per column.\n",
        "    Outputs\n",
        "      Graph Fourier Transform [numpy.ndarray]\n",
        "    \"\"\"\n",
        "    return U.T @ signal\n",
        "\n",
        "def iGFT(fourier_coefficients: np.ndarray, U: np.ndarray):\n",
        "    \"\"\" \n",
        "    Inputs\n",
        "      fourier_coefficients [numpy.ndarray]: float array of size n, containing a \n",
        "                                            signal represented in the spectral domain\n",
        "      U [numpy.ndarray]: matrix of size n x n containing one eigenvector per column.\n",
        "    Outputs\n",
        "      Inverse Graph Fourier Transform [numpy.ndarray]\n",
        "    \"\"\"\n",
        "    return U @ fourier_coefficients\n",
        "\n",
        "## function to filter graph signal with defined filter\n",
        "def filter_signal(x: np.array, spectral_response: np.array, U: np.ndarray):\n",
        "    \"\"\" \n",
        "    Returns a filtered signal. The filter is defined in the spectral domain by its \n",
        "    value on each eigenvector\n",
        "\n",
        "    Inputs\n",
        "      x [numpy.ndarray]: input signal\n",
        "      spectral response [numpy.ndarray]: value of the filter at each eigenvalue\n",
        "      U (n x n matrix): eigenvectors (one per column).\n",
        "    Outputs\n",
        "      out [numpy.ndarray]: Filtered signal\n",
        "    \"\"\"\n",
        "    x_gft = GFT(x, U)\n",
        "    filter_gft = x_gft * spectral_response\n",
        "    return iGFT(filter_gft, U)\n",
        "\n",
        "def compute_smoothness(x: np.array, laplacian: np.ndarray):\n",
        "    \"\"\" \n",
        "    Return the average smoothness of input graph signals\n",
        "\n",
        "    Inputs \n",
        "      x [numpy.ndarray]: Input signal\n",
        "      laplacian [numpy.ndarray] : Laplacian matrix\n",
        "    Outputs\n",
        "      Average smoothness of input graph signals\n",
        "    \"\"\" \n",
        "    smoothness = x.T @ laplacian @ x\n",
        "    return smoothness.diagonal()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH2xlYbIMqLr"
      },
      "source": [
        "### Eigenvalues and smoothness\n",
        "\n",
        "**Firstly, we plot the eigenvalues and analyse smoothness of graph signals.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1sNx2c6Sh_1"
      },
      "source": [
        "To calculate the smoothness of our features, we calculate the inverse of  smoothness along each feature. Then, we calculate the average of the smoothness of all features. The process is indicated as following:\n",
        "\n",
        "$$smoothness = \\sum^{i=N_{features}}_{i=1} \\frac{x_i^{T}Lx_i} {N_{features}}$$\n",
        "\n",
        "where ${N_{features}} = 35$ (number of hand-crafted features)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Marvel 2000**"
      ],
      "metadata": {
        "id": "TozJJ2_FUR3c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IM-PkzifQzRl"
      },
      "outputs": [],
      "source": [
        "# compute symmetric normalized graph laplacian and calculate smoothness for Marvel 2000\n",
        "L_sym_mar_2000 = compute_laplacian(adjacency_mar_2000, 'sym')\n",
        "smooth_mar_2000 = compute_smoothness(features_mar_2000, L_sym_mar_2000)\n",
        "lam_mar_2000, U_mar_2000 = spectral_decomposition(L_sym_mar_2000)\n",
        "lam_mar_2000, U_mar_2000 = np.real(lam_mar_2000), np.real(U_mar_2000.real)\n",
        "\n",
        "# sort eigenvalues\n",
        "argsort = lam_mar_2000.argsort()\n",
        "lam_mar_2000 = lam_mar_2000[argsort]\n",
        "U_mar_2000 = U_mar_2000[:, argsort]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Marvel 2001**"
      ],
      "metadata": {
        "id": "wS7ILniVUUnm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPu9IaCVW8gj"
      },
      "outputs": [],
      "source": [
        "# compute symmetric normalized graph laplacian and calculate smoothness for Marvel 2001\n",
        "L_sym_mar_2001 = compute_laplacian(adjacency_mar_2001, 'sym')\n",
        "smooth_mar_2001 = compute_smoothness(features_mar_2001, L_sym_mar_2001)\n",
        "lam_mar_2001, U_mar_2001 = spectral_decomposition(L_sym_mar_2001)\n",
        "lam_mar_2001, U_mar_2001 = np.real(lam_mar_2001), np.real(U_mar_2001.real)\n",
        "\n",
        "# sort eigenvalues\n",
        "argsort = lam_mar_2001.argsort()\n",
        "lam_mar_2001 = lam_mar_2001[argsort]\n",
        "U_mar_2001 = U_mar_2001[:, argsort]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DC 2000**"
      ],
      "metadata": {
        "id": "GYMQrchpUYTH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4XiPTDsW8i3"
      },
      "outputs": [],
      "source": [
        "# compute symmetric normalized graph laplacian and calculate smoothness for DC 2000\n",
        "L_sym_DC_2000 = compute_laplacian(adjacency_DC_2000, 'sym')\n",
        "smooth_DC_2000 = compute_smoothness(features_DC_2000, L_sym_DC_2000)\n",
        "lam_DC_2000, U_DC_2000 = spectral_decomposition(L_sym_DC_2000)\n",
        "lam_DC_2000, U_DC_2000 = np.real(lam_DC_2000), np.real(U_DC_2000.real)\n",
        "\n",
        "# sort eigenvalues\n",
        "argsort = lam_DC_2000.argsort()\n",
        "lam_DC_2000 = lam_DC_2000[argsort]\n",
        "U_DC_2000 = U_DC_2000[:, argsort]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DC 2001**"
      ],
      "metadata": {
        "id": "pu6OeYWSUa8J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wrt_wthpW8lT"
      },
      "outputs": [],
      "source": [
        "# compute symmetric normalized graph laplacian and calculate smoothness for DC 2001\n",
        "L_sym_DC_2001 = compute_laplacian(adjacency_DC_2001, 'sym')\n",
        "smooth_DC_2001 = compute_smoothness(features_DC_2001, L_sym_DC_2001)\n",
        "lam_DC_2001, U_DC_2001 = spectral_decomposition(L_sym_DC_2001)\n",
        "lam_DC_2001, U_DC_2001 = np.real(lam_DC_2001), np.real(U_DC_2001.real)\n",
        "\n",
        "# sort eigenvalues\n",
        "argsort = lam_DC_2001.argsort()\n",
        "lam_DC_2001 = lam_DC_2001[argsort]\n",
        "U_DC_2001 = U_DC_2001[:, argsort]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWBSa8cgXxk9"
      },
      "source": [
        "**Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiC4Gd_xW8oP"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(16, 16))\n",
        "\n",
        "axes[0][0].plot(lam_mar_2000, '+-')\n",
        "axes[0][0].set_title('Eigenvalues $L_{sym}$ for Marvel 2000')\n",
        "axes[0][0].set_ylabel('Eigenvalues')\n",
        "axes[0][0].set_xlabel('Index')\n",
        "\n",
        "axes[0][1].plot(lam_mar_2001, '+-')\n",
        "axes[0][1].set_title('Eigenvalues $L_{sym}$ for Marvel 2001')\n",
        "axes[0][1].set_ylabel('Eigenvalues')\n",
        "axes[0][1].set_xlabel('Index')\n",
        "\n",
        "axes[1][0].plot(lam_DC_2000, '+-')\n",
        "axes[1][0].set_title('Eigenvalues $L_{sym}$ for DC 2000')\n",
        "axes[1][0].set_ylabel('Eigenvalues')\n",
        "axes[1][0].set_xlabel('Index')\n",
        "\n",
        "axes[1][1].plot(lam_DC_2001, '+-')\n",
        "axes[1][1].set_title('Eigenvalues $L_{sym}$ for DC 2001')\n",
        "axes[1][1].set_ylabel('Eigenvalues')\n",
        "axes[1][1].set_xlabel('Index')\n",
        "\n",
        "print('Average inverse of smoothness for Marvel 2000: {:.2f}'.format(smooth_mar_2000.mean()))\n",
        "print('Average inverse of smoothness for Marvel 2001: {:.2f}'.format(smooth_mar_2001.mean()))\n",
        "print('Average inverse of smoothness for DC 2000: {:.2f}'.format(smooth_DC_2000.mean()))\n",
        "print('Average inverse of smoothness for DC 2001: {:.2f}\\n\\n'.format(smooth_DC_2001.mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeQJBNQ_ZUj1"
      },
      "source": [
        "### Tikhonov filter\n",
        "\n",
        "From above figures and printings, we can see the graph signals are not smooth. Therefore, we will use a filter to only preserve the components associated with the smallest eigenvalues. **The Tikhonov filter** can reduce the components associated with large eigenvalues, and preserve the low frequencies. We will use it to **smooth the graph signals**. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Marvel 2000**"
      ],
      "metadata": {
        "id": "iMnxtA2WVsfQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqRQxPLIX184"
      },
      "outputs": [],
      "source": [
        "## define ideal Tikhonov filter\n",
        "alpha = 0.99 / np.max(lam_mar_2000)\n",
        "ideal_tk = 1 / (1 + alpha * lam_mar_2000)\n",
        "ideal_tk = ideal_tk[:, None]\n",
        "\n",
        "plt.plot(lam_mar_2000, ideal_tk, '+-')\n",
        "plt.title('Spectral response of Tikhonov filter for Marvel 2000')\n",
        "plt.xlabel('$\\lambda$')\n",
        "plt.ylabel('Spectral response')\n",
        "plt.savefig(os.path.join(figures_path, 'spectral_response_marvel_2000.png'))\n",
        "\n",
        "## filter graph signals\n",
        "filtered_features_mar_2000 = filter_signal(features_mar_2000, ideal_tk, U_mar_2000)\n",
        "\n",
        "filter_smooth_mar_2000 = compute_smoothness(filtered_features_mar_2000, L_sym_mar_2000)\n",
        "\n",
        "print('Before smoothing, the average inverse of smoothness of Marvel 2000: {:.2f}'.format(smooth_mar_2000.mean()))\n",
        "print('After smoothing, the average inverse of smoothness of Marvel 2000: {:.2f}\\n\\n'.format(filter_smooth_mar_2000.mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Marvel 2001**"
      ],
      "metadata": {
        "id": "CsGjZd8wWBFx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iwk0g3W_cHlc"
      },
      "outputs": [],
      "source": [
        "## define ideal Tikhonov filter\n",
        "alpha = 0.99 / np.max(lam_mar_2001)\n",
        "ideal_tk = 1 / (1 + alpha * lam_mar_2001)\n",
        "ideal_tk = ideal_tk[:, None]\n",
        "\n",
        "plt.plot(lam_mar_2001, ideal_tk, '+-')\n",
        "plt.title('Spectral response of Tikhonov filter for Marvel 2001')\n",
        "plt.xlabel('$\\lambda$')\n",
        "plt.ylabel('Spectral response')\n",
        "plt.savefig(os.path.join(figures_path, 'spectral_response_marvel_2001.png'))\n",
        "\n",
        "## filter graph signals\n",
        "filtered_features_mar_2001 = filter_signal(features_mar_2001, ideal_tk, U_mar_2001)\n",
        "\n",
        "filter_smooth_mar_2001 = compute_smoothness(filtered_features_mar_2001, L_sym_mar_2001)\n",
        "print('Before smoothing, the average inverse of smoothness of Marvel 2001: {:.2f}'.format(smooth_mar_2001.mean()))\n",
        "print('After smoothing, the average inverse of smoothness of Marvel 2001: {:.2f}\\n\\n'.format(filter_smooth_mar_2001.mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DC 2000**"
      ],
      "metadata": {
        "id": "_xPjacTQWQgi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clY3on6_cHnd"
      },
      "outputs": [],
      "source": [
        "## define ideal Tikhonov filter\n",
        "alpha = 0.99 / np.max(lam_DC_2000)\n",
        "ideal_tk = 1 / (1 + alpha * lam_DC_2000)\n",
        "ideal_tk = ideal_tk[:, None]\n",
        "\n",
        "plt.plot(lam_DC_2000, ideal_tk, '+-')\n",
        "plt.title('Spectral response of Tikhonov filter for DC 2000')\n",
        "plt.xlabel('$\\lambda$')\n",
        "plt.ylabel('Spectral response')\n",
        "plt.savefig(os.path.join(figures_path, 'spectral_response_DC_2000.png'))\n",
        "\n",
        "## filter graph signals\n",
        "filtered_features_DC_2000 = filter_signal(features_DC_2000, ideal_tk, U_DC_2000)\n",
        "\n",
        "filter_smooth_DC_2000 = compute_smoothness(filtered_features_DC_2000, L_sym_DC_2000)\n",
        "print('Before smoothing, the average inverse of smoothness of DC 2000: {:.2f}'.format(smooth_DC_2000.mean()))\n",
        "print('After smoothing, the average inverse of smoothness of DC 2000: {:.2f}\\n\\n'.format(filter_smooth_DC_2000.mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DC 2001**"
      ],
      "metadata": {
        "id": "XMJIxkUJWTCN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxSgmbNzcHpx"
      },
      "outputs": [],
      "source": [
        "## define ideal Tikhonov filter\n",
        "alpha = 0.99 / np.max(lam_DC_2001)\n",
        "ideal_tk = 1 / (1 + alpha * lam_DC_2001)\n",
        "ideal_tk = ideal_tk[:, None]\n",
        "\n",
        "plt.plot(lam_DC_2001, ideal_tk, '+-')\n",
        "plt.title('Spectral response of Tikhonov filter for DC 2001')\n",
        "plt.xlabel('$\\lambda$')\n",
        "plt.ylabel('Spectral response')\n",
        "plt.savefig(os.path.join(figures_path, 'spectral_response_DC_2001.png'))\n",
        "\n",
        "## filter graph signals\n",
        "filtered_features_DC_2001 = filter_signal(features_DC_2001, ideal_tk, U_DC_2001)\n",
        "\n",
        "filter_smooth_DC_2001 = compute_smoothness(filtered_features_DC_2001, L_sym_DC_2001)\n",
        "print('Before smoothing, the average inverse of smoothness of DC 2001: {:.2f}'.format(smooth_DC_2001.mean()))\n",
        "print('After smoothing, the average inverse of smoothness of DC 2001: {:.2f}\\n\\n'.format(filter_smooth_DC_2001.mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSpNjYGTsD0A"
      },
      "source": [
        "## Clustering (K-means)\n",
        "\n",
        "As a first step, we try unsupervised methods to see whether clustering algorithms can determine a character is good, neutral or bad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtDp96TgsFRQ"
      },
      "outputs": [],
      "source": [
        "def kmeansCluster(features, targets):\n",
        "    \"\"\"\n",
        "    Performs Kmeans clustering on the provided dataset. Then prints the resulting\n",
        "    accuracy and then visualises the dataset firtly with original labels and then\n",
        "    with the predicted labels\n",
        "    Inputs\n",
        "      features [numpy.ndarray]: NxD Train matrix where N is the number of nodes and \n",
        "                                D the number of features\n",
        "      labels [numpy.ndarray]: Nx1 Label vect where each node is labeld with 0,1 or 2\n",
        "                              depending wether they are good, bad or neutral.\n",
        "    \"\"\"\n",
        "    y_pred = KMeans(n_clusters=3, random_state=0).fit_predict(features)\n",
        "    print('Test accuracy: {}'.format((y_pred == targets).mean()))\n",
        "\n",
        "    print('True lables')\n",
        "    visualize_features(features, targets)\n",
        "\n",
        "    print('K-means results')\n",
        "    visualize_features(features, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeFEj25uvL7k"
      },
      "outputs": [],
      "source": [
        "kmeansCluster(features_mar_2000, targets_mar_2000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kmeansCluster(features_mar_2001, targets_mar_2001)"
      ],
      "metadata": {
        "id": "ZkBYNl2O3-t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeansCluster(features_DC_2000, targets_DC_2000)"
      ],
      "metadata": {
        "id": "lPICFp9E4D73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeansCluster(features_DC_2001, targets_DC_2001)"
      ],
      "metadata": {
        "id": "oZkSVZ404EIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there are only 3 classes we expect in the worst case scenario a 30% classification accuracy. The results of this method are too close to that and sometimes even worse. This can also be seen by comparing the plots with the original labels and the predicted labels. Therefore we can conclude that this method is not good enough for our goals."
      ],
      "metadata": {
        "id": "ZiMVcix2XfEC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wuKMsN8MTS-"
      },
      "source": [
        "## Node classification - Classical ML\n",
        "\n",
        "In this section we will perform the following methods in order to try to classify each node/character as good, bad or neutral :\n",
        "\n",
        "- Linear SVM (w/wout Tikhonov regularization) \n",
        "- SVM with RBF kernel (w/wout Tikhonov regularization) \n",
        "- Logistic regression (w/wout Tikhonov regularization) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classifier\n",
        "\n",
        "In order to do these we create the following classifier function responsible to perform cross validation or train the a model using the provided classifier type."
      ],
      "metadata": {
        "id": "7mEQQmofb0Wd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epmF6EfzlGkW"
      },
      "outputs": [],
      "source": [
        "from stellargraph.core.validation import separated\n",
        "from numpy.core.numeric import cross\n",
        "def train_classifier(features, targets, train_mask, test_mask, feature_selection = False, regularization=10.0, cross_validation=True, classifier_type='SVC'):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "    features [numpy.ndarray]: NxD Train matrix where N is the number of nodes and \n",
        "                            D the number of features\n",
        "    targets [numpy.ndarray]: Nx1 Label vect where each node is labeld with 0,1 or 2\n",
        "                            depending wether they are good, bad or neutral.\n",
        "\n",
        "    train_mask [numpy.ndarray]: NxD Train mask filled with True/False indicating\n",
        "                                which elements are to be used for training or not\n",
        "    test_mask [numpy.ndarray]: Nx1 Test mask filled with True/False indicating\n",
        "                               which elements are to be used for testing or not\n",
        "    feature_selection [bool]: Flag for weather to use feature selction, default is \n",
        "                              False. If an integer number is given than it is used as \n",
        "                              the number of highest ranked features to use\n",
        "    regularization [float]: Regularization parameter\n",
        "    cross__validation [bool]: If True the classifier performs a cv with scope to\n",
        "                              return the best regularization parameter. If False\n",
        "                              the classifier trains the whole dataset normally.\n",
        "    classifier_type [str]: Classifier to use. Possible classifiers : \n",
        "                            Linear SVM          -> \"LinearSVC\"\n",
        "                            RBF kernel SVM      -> \"SVC\"\n",
        "                            Logistic regression -> \"Logistic\"\n",
        "\n",
        "  Outputs:\n",
        "     confmat: confusion matrix\n",
        "     scores: feature ranking scores\n",
        "  \"\"\"\n",
        "\n",
        "  seed = 0\n",
        "  # split the data into training and testing sets\n",
        "  X_train, X_test, y_train, y_test = features[train_mask, :], features[test_mask, :], targets[train_mask], targets[test_mask]\n",
        "\n",
        "  # build and train the ML model\n",
        "  if feature_selection:\n",
        "    selector = SelectKBest(f_classif, k=feature_selection)\n",
        "    X_train = selector.fit_transform(X_train, y_train)\n",
        "    features = selector.get_support(indices=True)\n",
        "    scores = selector.scores_\n",
        "    X_test = X_test[:,features]\n",
        "  \n",
        "  if classifier_type == 'SVC':\n",
        "      model = SVC(C=regularization, random_state=seed, tol=1e-5)\n",
        "  elif classifier_type == 'LinearSVC':\n",
        "      model = LinearSVC(C=regularization, random_state=seed, tol=1e-5, max_iter=2000)\n",
        "  elif classifier_type == 'Logistic':\n",
        "      model = LogisticRegression(C=regularization, solver='liblinear', random_state=seed, tol=1e-5)\n",
        "  elif classifier_type == 'rf':\n",
        "      model = RandomForestClassifier(n_estimators=100, max_depth=20, criterion='gini', class_weight = 'balanced', oob_score = True, random_state = seed)\n",
        "\n",
        "  \n",
        "  if cross_validation:\n",
        "      params = {'C':[0.5, 1.0, 5.0, 10.0, 15.0]}\n",
        "      kf = StratifiedKFold(n_splits=5, shuffle=True, random_state = seed) #set random_state to be 0\n",
        "      clf = GridSearchCV(model, params, scoring='f1_macro', n_jobs=-1, cv=kf)\n",
        "      clf.fit(X_train, y_train)\n",
        "      print('Best hyperparameter:')\n",
        "      print(clf.best_params_)\n",
        "  else:\n",
        "      print('The result of {}'.format(classifier_type))\n",
        "      model.fit(X_train, y_train)\n",
        "\n",
        "      # use the model to predict the labels of the test data\n",
        "      pre_test = model.predict(X_test)\n",
        "      confmat = confusion_matrix(y_test, pre_test, normalize='true')\n",
        "      print('Test accuracy: {:.2f}'.format(np.mean(pre_test==y_test)))\n",
        "      print('F1 score: {:.2f}'.format(f1_score(y_test, pre_test, average='macro')))\n",
        "\n",
        "      # Display the confusion matrix\n",
        "      plotConfusion(confmat)\n",
        "      if feature_selection:\n",
        "        return confmat, scores\n",
        "      else:\n",
        "        return confmat\n",
        "\n",
        "def plotConfusion(confmat):\n",
        "    \"\"\"\n",
        "    Plots the confusion matrix based on the predictions of our model\n",
        "    \"\"\"\n",
        "    classes = ['Bad', 'Neutral', 'Good']\n",
        "    df_cm = pd.DataFrame(confmat, index = classes, columns = classes)\n",
        "\n",
        "    plt.figure(figsize = (12,7))\n",
        "    sns.heatmap(df_cm, annot=True, cmap='Blues')\n",
        "    plt.title('Confusion Matrix')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have everything we need to go on and perform classical machine learning classification methods on our datasets."
      ],
      "metadata": {
        "id": "mD8u0GipdSii"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7wNYlcnUM1I"
      },
      "source": [
        "### Marvel 2000 - Classic ML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAw7uBwdXm_a"
      },
      "source": [
        "**Linear SVM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5BTRsiGX90Z"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-qwPBJgXp4y"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_mar_2000, targets_mar_2000, train_mask_mar_2000, test_mask_mar_2000, regularization=10.0, cross_validation=True, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kphPqn7LXmOy"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_mar_2000, targets_mar_2000, train_mask_mar_2000, test_mask_mar_2000, regularization=10.0, cross_validation=False, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDry_SQMdsiF"
      },
      "source": [
        "**Linear SVM with Tikhonov regularization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkOkeAE_d1TU"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_mar_2000, targets_mar_2000, train_mask_mar_2000, test_mask_mar_2000, regularization=10.0, cross_validation=True, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeJWQ5wed69D"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_mar_2000, targets_mar_2000, train_mask_mar_2000, test_mask_mar_2000, regularization=10.0, cross_validation=False, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq-z19lfYhmB"
      },
      "source": [
        "**SVM with RBF kernel**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orchmXwmYotT"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMSb3PXHYqJi"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_mar_2000, targets_mar_2000, train_mask_mar_2000, test_mask_mar_2000, regularization=10.0, cross_validation=True, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_ZyheKNlGrP"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_mar_2000, targets_mar_2000, train_mask_mar_2000, test_mask_mar_2000, regularization=15.0, cross_validation=False, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3oOJKFUeDI8"
      },
      "source": [
        "**SVM with RBF kernel and graph Tikhonov regularization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geH_WJIYXu0p"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_mar_2000, targets_mar_2000, train_mask_mar_2000, test_mask_mar_2000, regularization=10.0, cross_validation=True, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGAR8PNNeLCk"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_mar_2000, targets_mar_2000, train_mask_mar_2000, test_mask_mar_2000, regularization=15.0, cross_validation=False, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2k_F7XtYzi7"
      },
      "source": [
        "**Logistic regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpJXp5DVZEwJ"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCHb0wrCY-6i"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_mar_2000, targets_mar_2000, train_mask_mar_2000, test_mask_mar_2000, regularization=10.0, cross_validation=True, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdw77OKTY-9J"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_mar_2000, targets_mar_2000, train_mask_mar_2000, test_mask_mar_2000, regularization=10.0, cross_validation=False, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3730N63eQ4c"
      },
      "source": [
        "**Logistic regression with Tikhonov regularization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nolEOhEoeSqE"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_mar_2000, targets_mar_2000, train_mask_mar_2000, test_mask_mar_2000, regularization=10.0, cross_validation=True, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niCup3wyeWSj"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_mar_2000, targets_mar_2000, train_mask_mar_2000, test_mask_mar_2000, regularization=1.0, cross_validation=False, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT1ev18zZL87"
      },
      "source": [
        "### Marvel 2001 - Classic ML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp84evNtZPh0"
      },
      "source": [
        "**Linear SVM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn_lKeIRZTZy"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlrI5_6gY_GQ"
      },
      "outputs": [],
      "source": [
        "train_classifier(features_mar_2001, targets_mar_2001, train_mask_mar_2001, test_mask_mar_2001, regularization=10.0, cross_validation=True, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEKKiKAuY_JD"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_mar_2001, targets_mar_2001, train_mask_mar_2001, test_mask_mar_2001, regularization=1.0, cross_validation=False, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxhh6oLkehJd"
      },
      "source": [
        "**Linear SVM with Tikhonov regularization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6K0t1w_UemVU"
      },
      "outputs": [],
      "source": [
        "train_classifier(filtered_features_mar_2001, targets_mar_2001, train_mask_mar_2001, test_mask_mar_2001, regularization=10.0, cross_validation=True, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9n-fpt3eopM"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_mar_2001, targets_mar_2001, train_mask_mar_2001, test_mask_mar_2001, regularization=5.0, cross_validation=False, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baznw31JZjJ7"
      },
      "source": [
        "**SVM with RBF kernel**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIrj6gmrZpdZ"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHC1xkM7Y_Lp"
      },
      "outputs": [],
      "source": [
        "train_classifier(features_mar_2001, targets_mar_2001, train_mask_mar_2001, test_mask_mar_2001, regularization=10.0, cross_validation=True, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3NHakM7ZmQk"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_mar_2001, targets_mar_2001, train_mask_mar_2001, test_mask_mar_2001, regularization=5.0, cross_validation=False, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMCIpyR_eutJ"
      },
      "source": [
        "**SVM with RBF kernel and Tikhonov regularization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr4LiKIyewYU"
      },
      "outputs": [],
      "source": [
        "train_classifier(filtered_features_mar_2001, targets_mar_2001, train_mask_mar_2001, test_mask_mar_2001, regularization=10.0, cross_validation=True, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlYd-eV7ezf0"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_mar_2001, targets_mar_2001, train_mask_mar_2001, test_mask_mar_2001, regularization=5.0, cross_validation=False, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyLVhORpaB9c"
      },
      "source": [
        "**Logistic regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls9-eT51aRMc"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL7GxJPPZmSz"
      },
      "outputs": [],
      "source": [
        "train_classifier(features_mar_2001, targets_mar_2001, train_mask_mar_2001, test_mask_mar_2001, regularization=10.0, cross_validation=True, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKJ4NYG4ZmUi"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_mar_2001, targets_mar_2001, train_mask_mar_2001, test_mask_mar_2001, regularization=10.0, cross_validation=False, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv-vhpO3e6ds"
      },
      "source": [
        "**Logistic regression with Tikhonov regularization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yrRqwPLe30k"
      },
      "outputs": [],
      "source": [
        "train_classifier(filtered_features_mar_2001, targets_mar_2001, train_mask_mar_2001, test_mask_mar_2001, regularization=10.0, cross_validation=True, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwMq2O3qe-kE"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_mar_2001, targets_mar_2001, train_mask_mar_2001, test_mask_mar_2001, regularization=1.0, cross_validation=False, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjByn2gYar7y"
      },
      "source": [
        "### DC 2000 - Classic ML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CCQ20NmauWL"
      },
      "source": [
        "**Linear SVM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTWGWv38aw0B"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp8qOCbjZmWq"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_DC_2000, targets_DC_2000, train_mask_DC_2000, test_mask_DC_2000, regularization=10.0, cross_validation=True, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBEnGMB1ZmYq"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_DC_2000, targets_DC_2000, train_mask_DC_2000, test_mask_DC_2000, regularization=10.0, cross_validation=False, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXMMAs1QfEs2"
      },
      "source": [
        "**Linear SVM with Tikhonov regularization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wVtFGivfFKU"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_DC_2000, targets_DC_2000, train_mask_DC_2000, test_mask_DC_2000, regularization=10.0, cross_validation=True, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WbFOYjOfHb0"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_DC_2000, targets_DC_2000, train_mask_DC_2000, test_mask_DC_2000, regularization=10.0, cross_validation=False, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp8oequua89D"
      },
      "source": [
        "**SVM with RBF kernel**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRcp3o6Ja_wa"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Mgsj5heZmap"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_DC_2000, targets_DC_2000, train_mask_DC_2000, test_mask_DC_2000, regularization=10.0, cross_validation=True, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHzy7T0QZmcs"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_DC_2000, targets_DC_2000, train_mask_DC_2000, test_mask_DC_2000, regularization=10.0, cross_validation=False, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYw-betefMHs"
      },
      "source": [
        "**SVM with RBF kernel and Tikhonov regularization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MiErBrvfNmd"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_DC_2000, targets_DC_2000, train_mask_DC_2000, test_mask_DC_2000, regularization=10.0, cross_validation=True, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJ9EHt9TfRff"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_DC_2000, targets_DC_2000, train_mask_DC_2000, test_mask_DC_2000, regularization=15.0, cross_validation=False, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg29tGhXbIAq"
      },
      "source": [
        "**Logistic regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioWqIxP5bKLq"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUuO_KqnZmeq"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_DC_2000, targets_DC_2000, train_mask_DC_2000, test_mask_DC_2000, regularization=10.0, cross_validation=True, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6LBvupSZmgp"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_DC_2000, targets_DC_2000, train_mask_DC_2000, test_mask_DC_2000, regularization=5.0, cross_validation=False, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs4rHoUMfcwj"
      },
      "source": [
        "**Logistic regression with Tikhonov regularization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpRAYKsofe4g"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_DC_2000, targets_DC_2000, train_mask_DC_2000, test_mask_DC_2000, regularization=10.0, cross_validation=True, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-e46-Bn4fhsk"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_DC_2000, targets_DC_2000, train_mask_DC_2000, test_mask_DC_2000, regularization=10.0, cross_validation=False, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCj-GTwUbXDB"
      },
      "source": [
        "### DC 2001 - Classic ML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b23SMo6ybY1S"
      },
      "source": [
        "**Linear SVM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_q0eCyLbd5t"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QcnF1GKZmi7"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_DC_2001, targets_DC_2001, train_mask_DC_2001, test_mask_DC_2001, regularization=0.5, cross_validation=True, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7pX23YDbt4S"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_DC_2001, targets_DC_2001, train_mask_DC_2001, test_mask_DC_2001, regularization=1.0, cross_validation=False, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gThcVIlkfmvA"
      },
      "source": [
        "**Linear SVM with Tikhonov regularization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBUm-8eKfpFm"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_DC_2001, targets_DC_2001, train_mask_DC_2001, test_mask_DC_2001, regularization=0.5, cross_validation=True, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znz3RshLfrvk"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_DC_2001, targets_DC_2001, train_mask_DC_2001, test_mask_DC_2001, regularization=0.5, cross_validation=False, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rdk7siN8b1Pb"
      },
      "source": [
        "**SVM with RBF kernel**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAZLNV-Fb-0z"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lCOpsMtb3rb"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_DC_2001, targets_DC_2001, train_mask_DC_2001, test_mask_DC_2001, regularization=0.5, cross_validation=True, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZP0G4mzb7tM"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_DC_2001, targets_DC_2001, train_mask_DC_2001, test_mask_DC_2001, regularization=5.0, cross_validation=False, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5S63KN6fygV"
      },
      "source": [
        "**SVM with RBF kernel and Tikhonov regularization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPLPjwQBfy7-"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_DC_2001, targets_DC_2001, train_mask_DC_2001, test_mask_DC_2001, regularization=0.5, cross_validation=True, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TppRanXcf1gt"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_DC_2001, targets_DC_2001, train_mask_DC_2001, test_mask_DC_2001, regularization=10.0, cross_validation=False, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsaG-rr5cBnb"
      },
      "source": [
        "**Logistic regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXbnVAgqcD0L"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrWdBt4TcFOx"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_DC_2001, targets_DC_2001, train_mask_DC_2001, test_mask_DC_2001, regularization=0.5, cross_validation=True, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK71Cji9oC9i"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(features_DC_2001, targets_DC_2001, train_mask_DC_2001, test_mask_DC_2001, regularization=5.0, cross_validation=False, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S_KmPI8f_nV"
      },
      "source": [
        "**Logistic regression with Tikhonov regularization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmTddVpngBk9"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_DC_2001, targets_DC_2001, train_mask_DC_2001, test_mask_DC_2001, regularization=0.5, cross_validation=True, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEnL4jtugESt"
      },
      "outputs": [],
      "source": [
        "_ = train_classifier(filtered_features_DC_2001, targets_DC_2001, train_mask_DC_2001, test_mask_DC_2001, regularization=5.0, cross_validation=False, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK1Ewl8isPHi"
      },
      "source": [
        "## Node embedding (GNN, GTA)\n",
        "\n",
        "In order to improve our results we will now use Graph Neural Networks instead of Classical Machine Learning to classify our characters. In this section we will define the building blocks, models and all revelant functions that we will later use to train and evaluate our results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC4lrxTYkcpe"
      },
      "outputs": [],
      "source": [
        "# Check for a GPU -  Graphical Processing Unit\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU :(\")\n",
        "    device = 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Blocks\n"
      ],
      "metadata": {
        "id": "7h66YT-UeP8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GNN Blocks :"
      ],
      "metadata": {
        "id": "-Q87wtnfhC6b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zc2XkzkdoDGv"
      },
      "outputs": [],
      "source": [
        "class GNNBlock1(nn.Module):\n",
        "    def __init__(self, nb_features: int, embedding_dim: int, drop_out: float) -> None:\n",
        "        super().__init__()\n",
        "        self.conv1 = pyg.nn.GraphConv(nb_features, embedding_dim)\n",
        "        \n",
        "        if drop_out:\n",
        "            self.dropout = nn.Dropout(p=drop_out)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        if self.dropout:\n",
        "            x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5XqNh2WFcIW"
      },
      "outputs": [],
      "source": [
        "class GNNBlock2(nn.Module):\n",
        "    def __init__(self, nb_features: int, embedding_dim: int, drop_out: float) -> None:\n",
        "        super().__init__()\n",
        "        self.conv1 = pyg.nn.GATConv(nb_features, embedding_dim)\n",
        "        \n",
        "        if drop_out:\n",
        "            self.dropout = nn.Dropout(p=drop_out)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        if self.dropout:\n",
        "            x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYa2T3nCtXkW"
      },
      "outputs": [],
      "source": [
        "class GNNBlock3(nn.Module):\n",
        "    def __init__(self, nb_features: int, embedding_dim: int, drop_out: float) -> None:\n",
        "        super().__init__()\n",
        "        self.conv1 = pyg.nn.ChebConv(nb_features, embedding_dim, K=7)\n",
        "        \n",
        "        if drop_out:\n",
        "            self.dropout = nn.Dropout(p=drop_out)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        if self.dropout:\n",
        "            x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2QpDcqiNOGa"
      },
      "outputs": [],
      "source": [
        "class GNNBlock4(nn.Module):\n",
        "    def __init__(self, nb_features: int, embedding_dim: int, drop_out: float) -> None:\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(nb_features, embedding_dim)\n",
        "        self.conv1 = pyg.nn.GINConv(self.linear)\n",
        "        \n",
        "        if drop_out:\n",
        "            self.dropout = nn.Dropout(p=drop_out)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        if self.dropout:\n",
        "            x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Architecture :"
      ],
      "metadata": {
        "id": "qKpShodAgyP4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzCtAe9goDJt"
      },
      "outputs": [],
      "source": [
        "class GNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Puts the same building block 5 times in series, each time increasing the resolution\n",
        "    \"\"\"\n",
        "    def __init__(self, GNNBlock, in_features: int, embedding_dim: int, hidden_dims=[32, 64, 128, 256], drop_pro=0.3) -> None:\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        layers.append(GNNBlock(in_features, hidden_dims[0], drop_pro))\n",
        "        for i, hidden_dim in enumerate(hidden_dims[1:]):\n",
        "            layers.append(GNNBlock(hidden_dims[i], hidden_dim, drop_pro))\n",
        "        layers.append(GNNBlock(hidden_dims[-1], embedding_dim, drop_pro))\n",
        "        self.gnn = nn.ModuleList(layers)      \n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for layer in self.gnn:\n",
        "            x = layer(x, edge_index)        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOm3mveqhwUr"
      },
      "outputs": [],
      "source": [
        "class NodeClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Applies firstly a provided GNN block and then passes the input through a fully\n",
        "    connected linear neural network.\n",
        "    \"\"\"\n",
        "    def __init__(self, gnn_block: nn.Module, embedding_dim: int, num_classes: int) -> None:\n",
        "        super().__init__()\n",
        "        self.gnn_block = gnn_block\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(embedding_dim, num_classes),\n",
        "            nn.Dropout(p=0.1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index) -> torch.Tensor:\n",
        "        x = self.gnn_block(x, edge_index)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relevant functions :"
      ],
      "metadata": {
        "id": "6mTUGcVZg2Dh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LMe5-80kOyX"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "\n",
        "    model: nn.Module,\n",
        "    data: Data,\n",
        "    loss_fn: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    nb_epochs: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    Training our model\n",
        "    \"\"\"\n",
        "    for epoch in tqdm(range(nb_epochs), total=nb_epochs):\n",
        "        ## get predicted output\n",
        "        output = model(data.x, data.edge_index)\n",
        "        ## calculate loss\n",
        "        loss = loss_fn(output[data.train_mask,:], data.y[data.train_mask]) \n",
        "               \n",
        "        ## optimizer updates\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    #^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpobsjtXkSmv"
      },
      "outputs": [],
      "source": [
        "def evaluate(model: nn.Module, metric: torchmetrics.Metric, data: Data, mask: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Evaluates our model\n",
        "    \"\"\"\n",
        "    model.eval()  # Deactivate dropout\n",
        "    with torch.no_grad():\n",
        "        pred = model(data.x, data.edge_index).softmax(dim=1)\n",
        "        _ = metric(pred[mask, :], data.y[mask])        \n",
        "        #^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "    return metric.compute().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-SnD7t0Yw-1"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix_f1_score(model: nn.Module, data: Data, mask: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Plots the confusion matrix with the f1 score instead of accuracy\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    label = data.y[mask]\n",
        "    y_pred, y_true = [], label.data.cpu().numpy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred = model(data.x, data.edge_index).softmax(dim=1)\n",
        "        _, predicted = pred.max(1)\n",
        "\n",
        "        test_pred = predicted.data.cpu().numpy()\n",
        "        y_pred = test_pred[mask.data.cpu().numpy()]\n",
        "    \n",
        "\n",
        "    classes = ['Bad', 'Neutral', 'Good']\n",
        "    c_matrix = confusion_matrix(y_true, y_pred, normalize='true')\n",
        "    df_cm = pd.DataFrame(c_matrix, index = classes, columns = classes)\n",
        "\n",
        "    plt.figure(figsize = (12,7))\n",
        "    print('F1 score: {}'.format(f1_score(y_true, y_pred, average='macro')))\n",
        "    sns.heatmap(df_cm, annot=True, cmap='Blues')\n",
        "    plt.title('Confusion Matrix')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example\n",
        "\n",
        "Simply showing an example of what we did so far."
      ],
      "metadata": {
        "id": "qE-sgBejggcc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMvhrRuToDMR"
      },
      "outputs": [],
      "source": [
        "##### test model\n",
        "gnn = GNN(GNNBlock1, 3, 512, drop_pro=0.5)\n",
        "x = torch.randn(6,3)\n",
        "edge_index = torch.arange(6, dtype=torch.long).reshape(2,3)\n",
        "output = gnn(x, edge_index)\n",
        "assert output.shape == (6, 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc-kLATpgNkt"
      },
      "outputs": [],
      "source": [
        "print(gnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debugging"
      ],
      "metadata": {
        "id": "T97kgcy1gpd1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN0XW3o3lrcv"
      },
      "outputs": [],
      "source": [
        "### make the empty current alias be empty string, because if we do not make the change,\n",
        "### a bug ('TypeError: new(): invalid data type 'str'') will occur.\n",
        "def change_empty_current_alias(graph):\n",
        "    for node in graph.nodes:\n",
        "        if graph.nodes[node]['Current Alias'] is '':\n",
        "            graph.nodes[node]['Current Alias'] = ' '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8NDgjuKsTxO"
      },
      "outputs": [],
      "source": [
        "### make the empty current alias be empty string, because if we do not make the change,\n",
        "### a bug ('TypeError: new(): invalid data type 'str'') will occur.\n",
        "change_empty_current_alias(graph_mar_2000)\n",
        "change_empty_current_alias(graph_mar_2001)\n",
        "change_empty_current_alias(graph_DC_2000)\n",
        "change_empty_current_alias(graph_DC_2001)\n",
        "\n",
        "\n",
        "pyg_graph_mar_2000 = from_networkx(graph_mar_2000)\n",
        "pyg_graph_mar_2001 = from_networkx(graph_mar_2001)\n",
        "pyg_graph_DC_2000 = from_networkx(graph_DC_2000)\n",
        "pyg_graph_DC_2001 = from_networkx(graph_DC_2001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7HYn792k4V3"
      },
      "outputs": [],
      "source": [
        "pyg_graph_mar_2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32RBdtRBlf0V"
      },
      "outputs": [],
      "source": [
        "pyg_graph_mar_2000.x = pyg_graph_mar_2000.x.to(torch.float32)\n",
        "pyg_graph_mar_2001.x = pyg_graph_mar_2001.x.to(torch.float32)\n",
        "pyg_graph_DC_2000.x = pyg_graph_DC_2000.x.to(torch.float32)\n",
        "pyg_graph_DC_2001.x = pyg_graph_DC_2001.x.to(torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdJ6PPc_nL-X"
      },
      "source": [
        "Tikhonov filtered graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsbZzhgQmeUH"
      },
      "outputs": [],
      "source": [
        "filtered_pyg_graph_mar_2000 = pyg_graph_mar_2000.clone()\n",
        "filtered_pyg_graph_mar_2000.x = torch.from_numpy(filtered_features_mar_2000).to(torch.float32)\n",
        "\n",
        "filtered_pyg_graph_mar_2001 = pyg_graph_mar_2001.clone()\n",
        "filtered_pyg_graph_mar_2001.x = torch.from_numpy(filtered_features_mar_2001).to(torch.float32)\n",
        "\n",
        "filtered_pyg_graph_DC_2000 = pyg_graph_DC_2000.clone()\n",
        "filtered_pyg_graph_DC_2000.x = torch.from_numpy(filtered_features_DC_2000).to(torch.float32)\n",
        "\n",
        "filtered_pyg_graph_DC_2001 = pyg_graph_DC_2001.clone()\n",
        "filtered_pyg_graph_DC_2001.x = torch.from_numpy(filtered_features_DC_2001).to(torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_-GQiAoko7v"
      },
      "source": [
        "## Train and evaluate GNN classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC4uIfdyg10A"
      },
      "source": [
        "### Marvel 2000 - GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzopFdThsT4Y"
      },
      "outputs": [],
      "source": [
        "# embedding dimensions\n",
        "EMBEDDING_DIM = 128\n",
        "\n",
        "# learning rate\n",
        "LR = 1e-2\n",
        "\n",
        "# weight decay\n",
        "WD = 1e-3\n",
        "\n",
        "gnn_mar_2000 = GNN(GNNBlock3, pyg_graph_mar_2000.num_features, EMBEDDING_DIM, hidden_dims=[8, 16, 32, 64], drop_pro=0.1).to(device)\n",
        "model = NodeClassifier(\n",
        "    gnn_mar_2000,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    num_classes=3\n",
        ").to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "nb_epochs = 1000\n",
        "\n",
        "data = pyg_graph_mar_2000.to(device)\n",
        "\n",
        "train(model, data, loss_fn, optimizer, nb_epochs)\n",
        "\n",
        "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBg25Epsowly"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JL0ACZw5Ai2"
      },
      "outputs": [],
      "source": [
        "print('Result for Marvel 2000')\n",
        "accuracy_train = evaluate(model, torchmetrics.Accuracy().to(device), data, data.train_mask)\n",
        "print(\"Train accuracy:\", accuracy_train)\n",
        "print(\"Test accuracy:\", evaluate(model, torchmetrics.Accuracy().to(device), data, data.test_mask))\n",
        "\n",
        "confusion_matrix_f1_score(model, data, data.test_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_ULKEXRnG4y"
      },
      "outputs": [],
      "source": [
        "nodeEmbedding_mar_2000 = gnn_mar_2000(data.x, data.edge_index).data.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB-u4G9ynG7l"
      },
      "outputs": [],
      "source": [
        "#Targets visualisation\n",
        "visualize_features(features_mar_2000, targets_mar_2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y_bBMFnnHEe"
      },
      "outputs": [],
      "source": [
        "#Predictions visualisation\n",
        "visualize_features(nodeEmbedding_mar_2000, targets_mar_2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdrJYfN_nRRD"
      },
      "source": [
        "From above figures, we can see GNN can provide more meaningful features than hand-crafted features. It is very clear in TSNE plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA8EK7PLnYBH"
      },
      "source": [
        "**Using Tikhonov regularization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY6LBVJCnnQ-"
      },
      "outputs": [],
      "source": [
        "# embedding dimensions\n",
        "EMBEDDING_DIM = 128\n",
        "\n",
        "# learning rate\n",
        "LR = 1e-2\n",
        "\n",
        "# weight decay\n",
        "WD = 1e-3\n",
        "\n",
        "gnn_mar_2000 = GNN(GNNBlock3, filtered_pyg_graph_mar_2000.num_features, EMBEDDING_DIM, hidden_dims=[8, 16, 32, 64], drop_pro=0.1).to(device)\n",
        "model = NodeClassifier(\n",
        "    gnn_mar_2000,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    num_classes=3\n",
        ").to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "nb_epochs = 1000\n",
        "\n",
        "data = filtered_pyg_graph_mar_2000.to(device)\n",
        "\n",
        "train(model, data, loss_fn, optimizer, nb_epochs)\n",
        "\n",
        "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnx_9HA2nwIu"
      },
      "outputs": [],
      "source": [
        "print('Result for Marvel 2000')\n",
        "accuracy_train = evaluate(model, torchmetrics.Accuracy().to(device), data, data.train_mask)\n",
        "print(\"Train accuracy:\", accuracy_train)\n",
        "print(\"Test accuracy:\", evaluate(model, torchmetrics.Accuracy().to(device), data, data.test_mask))\n",
        "\n",
        "confusion_matrix_f1_score(model, data, data.test_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpp8tGjGig5_"
      },
      "source": [
        "### Marvel 2001 - GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YookBTf5AlO"
      },
      "outputs": [],
      "source": [
        "# embedding dimensions\n",
        "EMBEDDING_DIM = 128\n",
        "\n",
        "# learning rate\n",
        "LR = 1e-2\n",
        "\n",
        "# weight decay\n",
        "WD = 1e-3\n",
        "\n",
        "gnn_mar_2001 = GNN(GNNBlock3, pyg_graph_mar_2001.num_features, EMBEDDING_DIM, hidden_dims=[8, 16, 32, 64], drop_pro=0.1).to(device)\n",
        "model = NodeClassifier(\n",
        "    gnn_mar_2001,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    num_classes=3\n",
        ").to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "nb_epochs = 1000\n",
        "\n",
        "data = pyg_graph_mar_2001.to(device)\n",
        "\n",
        "train(model, data, loss_fn, optimizer, nb_epochs)\n",
        "\n",
        "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V3Kxw7YiuW3"
      },
      "outputs": [],
      "source": [
        "print('Result for Marvel 2001')\n",
        "accuracy_train = evaluate(model, torchmetrics.Accuracy().to(device), data, data.train_mask)\n",
        "print(\"Train accuracy:\", accuracy_train)\n",
        "print(\"Test accuracy:\", evaluate(model, torchmetrics.Accuracy().to(device), data, data.test_mask))\n",
        "\n",
        "confusion_matrix_f1_score(model, data, data.test_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMQAYrjDmxj8"
      },
      "outputs": [],
      "source": [
        "nodeEmbedding_mar_2001 = gnn_mar_2001(data.x, data.edge_index).data.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg5jNoM2m15b"
      },
      "outputs": [],
      "source": [
        "# Targets Visualisation\n",
        "visualize_features(features_mar_2001, targets_mar_2001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQaTun1Dm17x"
      },
      "outputs": [],
      "source": [
        "# Predictions visualisation\n",
        "visualize_features(nodeEmbedding_mar_2001, targets_mar_2001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM67IvxXnCad"
      },
      "source": [
        "From above figures, we can see GNN can provide more meaningful features than hand-crafted features. It is very clear in TSNE plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2vo6EfAjA6A"
      },
      "source": [
        "### DC 2000 - GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK5GaEs0i_-v"
      },
      "outputs": [],
      "source": [
        "# embedding dimensions\n",
        "EMBEDDING_DIM = 128\n",
        "\n",
        "# learning rate\n",
        "LR = 1e-2\n",
        "\n",
        "# weight decay\n",
        "WD = 1e-3\n",
        "\n",
        "gnn_dc_2000 = GNN(GNNBlock3, pyg_graph_DC_2000.num_features, EMBEDDING_DIM, hidden_dims=[8, 16, 32, 64], drop_pro=0.1).to(device)\n",
        "model = NodeClassifier(\n",
        "    gnn_dc_2000,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    num_classes=3\n",
        ").to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "nb_epochs = 1000\n",
        "\n",
        "data = pyg_graph_DC_2000.to(device)\n",
        "\n",
        "train(model, data, loss_fn, optimizer, nb_epochs)\n",
        "\n",
        "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-6BeQy6jAKa"
      },
      "outputs": [],
      "source": [
        "print('Result for DC 2000')\n",
        "accuracy_train = evaluate(model, torchmetrics.Accuracy().to(device), data, data.train_mask)\n",
        "print(\"Train accuracy:\", accuracy_train)\n",
        "print(\"Test accuracy:\", evaluate(model, torchmetrics.Accuracy().to(device), data, data.test_mask))\n",
        "\n",
        "confusion_matrix_f1_score(model, data, data.test_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcmAd7VZmZH4"
      },
      "outputs": [],
      "source": [
        "nodeEmbedding_dc_2000 = gnn_dc_2000(data.x, data.edge_index).data.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFGp-W61ma9t"
      },
      "outputs": [],
      "source": [
        "# Targets visualisation\n",
        "visualize_features(features_DC_2000, targets_DC_2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPdaR-8imbEC"
      },
      "outputs": [],
      "source": [
        "# Predictions visualisation\n",
        "visualize_features(nodeEmbedding_dc_2000, targets_DC_2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4glJHxeOmsC_"
      },
      "source": [
        "From above figures, we can see GNN can provide more meaningful features than hand-crafted features. It is very clear in TSNE plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHCYLF7di0Am"
      },
      "source": [
        "### DC 2001 - GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCFnJog-iy0o"
      },
      "outputs": [],
      "source": [
        "# embedding dimensions\n",
        "EMBEDDING_DIM = 128\n",
        "\n",
        "# learning rate\n",
        "LR = 1e-2\n",
        "\n",
        "# weight decay\n",
        "WD = 1e-3\n",
        "\n",
        "gnn_dc_2001 = GNN(GNNBlock3, pyg_graph_DC_2001.num_features, EMBEDDING_DIM, hidden_dims=[8, 16, 32, 64], drop_pro=0.1).to(device)\n",
        "model = NodeClassifier(\n",
        "    gnn_dc_2001,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    num_classes=3\n",
        ").to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "nb_epochs = 1000\n",
        "\n",
        "data = pyg_graph_DC_2001.to(device)\n",
        "\n",
        "train(model, data, loss_fn, optimizer, nb_epochs)\n",
        "\n",
        "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVZ_blVpiy8T"
      },
      "outputs": [],
      "source": [
        "print('Result for DC 2001')\n",
        "accuracy_train = evaluate(model, torchmetrics.Accuracy().to(device), data, data.train_mask)\n",
        "print(\"Train accuracy:\", accuracy_train)\n",
        "print(\"Test accuracy:\", evaluate(model, torchmetrics.Accuracy().to(device), data, data.test_mask))\n",
        "\n",
        "confusion_matrix_f1_score(model, data, data.test_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYmIdF-5izLV"
      },
      "outputs": [],
      "source": [
        "nodeEmbedding_dc_2001 = gnn_dc_2001(data.x, data.edge_index).data.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-sUdkpbmA5s"
      },
      "outputs": [],
      "source": [
        "# Targets visualisation\n",
        "visualize_features(features_DC_2001, targets_DC_2001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExD03oC9kGTR"
      },
      "outputs": [],
      "source": [
        "#Predictions visualisation\n",
        "visualize_features(nodeEmbedding_dc_2001, targets_DC_2001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-mOh9tMmJx8"
      },
      "source": [
        "From above figures, we can see GNN can provide more meaningful features than hand-crafted features. It is very clear in TSNE plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxdWUN2t7lQm"
      },
      "source": [
        "## Affiliation classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1uAA8BXCz7S"
      },
      "source": [
        "**The pipeline of the affiliation classification is demonstrated as following:**\n",
        "\n",
        "\n",
        "\n",
        "1. Assign labels for different affiliations (calculate the mean value of node labels) \n",
        "2. Pooling the node embeddings of each affiliation\n",
        "3. Train SVM classifier to do the classification \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_WppphcDVTT"
      },
      "source": [
        "### Assign labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjx9WhIoDXiU"
      },
      "outputs": [],
      "source": [
        "def transformLabel(x):\n",
        "    \"\"\"\n",
        "    Transforms mean value of labels of nodes in the following way\n",
        "      x in [-1,-0.33)    -> x=-1\n",
        "      x in [-0.33, 0.34) -> x=0\n",
        "      x in [0.34, 1]     -> x=1 \n",
        "\n",
        "    Inputs \n",
        "      x [float]: Mean value of feature \"Good\" for characters in specific\n",
        "                 affiliation\n",
        "    Outputs\n",
        "      Transformation of x as explained above\n",
        "    \"\"\"\n",
        "    if -1 <= x < -0.33:\n",
        "        return -1\n",
        "    elif -0.33 <= x < 0.34:\n",
        "        return 0\n",
        "    elif 0.34<= x <= 1:\n",
        "        return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDqEwvZQEGpu"
      },
      "outputs": [],
      "source": [
        "def assignLabels(df):\n",
        "    \"\"\"\n",
        "    Assigns a Good (1), Bad (-1) or Neutral (0) to each affiliation based on the \n",
        "    mean value of \"Good\" for the characters affiliated with it\n",
        "\n",
        "    Inputs\n",
        "      df [pandas.DataFrame]: Initial dataframe (parquet provided)\n",
        "    Outputs\n",
        "      df_affiliations [pandas.DataFrame] : Dataframe of the following form\n",
        "         \n",
        "                     |Affiliation|Label|Id\n",
        "    \"\"\"\n",
        "    df_nodes = df.explode('Affiliation')[['Id', 'Affiliation']]\n",
        "    df_affiliation = df.explode('Affiliation').groupby('Affiliation').mean().reset_index()[['Affiliation', 'Good']]\n",
        "    df_affiliation.rename(columns = {'Good': 'Label'}, inplace = True)\n",
        "    df_affiliation['Label'] = df_affiliation['Label'].apply(transformLabel)\n",
        "\n",
        "    df_affiliation = df_affiliation.merge(df_nodes, on='Affiliation')\n",
        "    return df_affiliation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRQFUoVyaf7q"
      },
      "outputs": [],
      "source": [
        "def checkTopAffLabels(df):\n",
        "    \"\"\"\n",
        "    Returns top 10 most popular affiliations and their label\n",
        "\n",
        "    Inputs\n",
        "      df [pandas.DataFrame]: Dataframe with the Affiliations labeled\n",
        "    \"\"\"\n",
        "    top_aff = df['Affiliation'].value_counts()[:10].index\n",
        "    for i, aff in enumerate(top_aff):\n",
        "        print('Top {} popular Affilation: {}, Label: {}'.format(i+1, aff, df[df['Affiliation'] == aff].Label.values[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTimeT49ZlNw"
      },
      "outputs": [],
      "source": [
        "# Affiliations labeling for Marvel 2000\n",
        "affiliation_mar_2000 = assignLabels(marvel_data_2000)\n",
        "# Affiliations labeling for Marvel 2001\n",
        "affiliation_mar_2001 = assignLabels(marvel_data_2001)\n",
        "# Affiliations labeling for DC 2000\n",
        "affiliation_DC_2000 = assignLabels(DC_data_2000)\n",
        "# Affiliations labeling for DC 2001\n",
        "affiliation_DC_2001 = assignLabels(DC_data_2001)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Showing an example of the created dataframes : "
      ],
      "metadata": {
        "id": "LjG31MEdmk6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "affiliation_mar_2000"
      ],
      "metadata": {
        "id": "nQVY4fGVla9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVTizC3zZ1IB"
      },
      "source": [
        "To check our assigned lables are meaningful, we verify the lables of the popular affiliations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loxdESPLaNe9"
      },
      "outputs": [],
      "source": [
        "checkTopAffLabels(affiliation_mar_2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVU6EVVhaDKE"
      },
      "outputs": [],
      "source": [
        "checkTopAffLabels(affiliation_mar_2001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu21ndeibiDC"
      },
      "source": [
        "**From above printing, we can see the our assigned labels are meaningful, because X-men and Avergers are indeed good affiliation (which everybody knows). Also, Masters of Evil is evil affiliation (which every body knows).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkEA6j3Jbbr5"
      },
      "outputs": [],
      "source": [
        "checkTopAffLabels(affiliation_DC_2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvLlRc1FaDMf"
      },
      "outputs": [],
      "source": [
        "checkTopAffLabels(affiliation_DC_2001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsCDHJQzcBmp"
      },
      "source": [
        "**From above printing, we can see the our assigned labels are meaningful, because Black Lantern Corps and All-Star Squadron are indeed good affiliation (which everybody knows). Also, Secret Society of Super-Villains III is evil affiliation (which every body knows).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybIT_UeqjzXI"
      },
      "source": [
        "**Label distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUfynFlQj2zo"
      },
      "outputs": [],
      "source": [
        "def plotAffLabelDistribution(df, axIndex, name='Marvel 2000'):\n",
        "    \"\"\"\n",
        "    Plots a barplot with the number of good, bad and neutral affiliations\n",
        "    in our dataset\n",
        "\n",
        "    Inputs \n",
        "      df [pandas.DataFrame] : Dataset dataframe\n",
        "      axIndex [matplotlib.axes._subplots.AxesSubplot] : Position of the graph\n",
        "      name [str] : Name of the graph to plot. Can be one of the following \n",
        "                   [\"Marvel 2000\", \"Marvel 2001\", \"DC 2000\", \"DC 2001\"]\n",
        "                   (just to give a title)\n",
        "    \"\"\"\n",
        "    labels = df['Label'].value_counts()\n",
        "    labels = labels.rename(index={1: 'Good', 0:'Neutral', -1:'Bad'})\n",
        "    ax = sns.barplot(x=labels.index, y=labels.values, ax=axIndex)\n",
        "    ax.set_title('{} Affiliation labels distribution'.format(name))\n",
        "    ax.set_ylabel('Number')\n",
        "    ax.set_xlabel('Class')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7z-bDW1j-xv"
      },
      "outputs": [],
      "source": [
        "sns.set_theme(style=\"whitegrid\")\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "plotAffLabelDistribution(affiliation_mar_2000, axes[0][0], name='Marvel 2000')\n",
        "plotAffLabelDistribution(affiliation_mar_2001, axes[0][1], name='Marvel 2001')\n",
        "plotAffLabelDistribution(affiliation_DC_2000, axes[1][0], name='DC 2000')\n",
        "plotAffLabelDistribution(affiliation_DC_2001, axes[1][1], name='DC 2001')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbQaFIoikaER"
      },
      "source": [
        "From above figures, we can see the labels are unbalanced. Therefore, we will split the data into train set and test set in stratified fashion. We will also report not only the test accuracy but also the **F1 score and confusion matrix**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMD_Ym9xcTcU"
      },
      "source": [
        "### Affiliation Embedding (Average pooling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q393W2BRLdZm"
      },
      "outputs": [],
      "source": [
        "def avgPooling(nodeEmbedding, df):\n",
        "    affiliation_list = df.Affiliation.unique()\n",
        "    aff_embedding = np.zeros((len(affiliation_list), nodeEmbedding.shape[1]))\n",
        "    labels = np.zeros(len(affiliation_list))\n",
        "\n",
        "    for i, aff in enumerate(affiliation_list):\n",
        "        mask = df[df['Affiliation'] == aff]['Id'].values\n",
        "        aff_embedding[i] = nodeEmbedding[mask, :].mean(axis=0)\n",
        "        labels[i] = int(df[df['Affiliation'] == aff]['Label'].values[0]) + 1\n",
        "    \n",
        "    return aff_embedding, labels, affiliation_list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Marvel 2000 Affiliations feature dataset**"
      ],
      "metadata": {
        "id": "in7mXCk2j_Jb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILNH_HdeZh1i"
      },
      "outputs": [],
      "source": [
        "aff_embedding_mar_2000, aff_labels_mar_2000, aff_mar_2000 = avgPooling(nodeEmbedding_mar_2000, affiliation_mar_2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVUV2A0ZdE4Z"
      },
      "outputs": [],
      "source": [
        "visualize_features(aff_embedding_mar_2000, aff_labels_mar_2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Marvel 2001 Affiliations feature dataset**"
      ],
      "metadata": {
        "id": "rAtPF13UkEfy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HUAOT07csPR"
      },
      "outputs": [],
      "source": [
        "aff_embedding_mar_2001, aff_labels_mar_2001, aff_mar_2001 = avgPooling(nodeEmbedding_mar_2001, affiliation_mar_2001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqOvZyMQdNDQ"
      },
      "outputs": [],
      "source": [
        "visualize_features(aff_embedding_mar_2001, aff_labels_mar_2001)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DC 2000 Affiliations feature dataset**"
      ],
      "metadata": {
        "id": "Fd2dPUOAkG3v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2nbfO2fcsRu"
      },
      "outputs": [],
      "source": [
        "aff_embedding_DC_2000, aff_labels_DC_2000, aff_DC_2000 = avgPooling(nodeEmbedding_dc_2000, affiliation_DC_2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGAATcXVddZ4"
      },
      "outputs": [],
      "source": [
        "visualize_features(aff_embedding_DC_2000, aff_labels_DC_2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DC 2001 Affiliations feature dataset**"
      ],
      "metadata": {
        "id": "yMX4Sn1DkJVE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cu3Frnf0csUU"
      },
      "outputs": [],
      "source": [
        "aff_embedding_DC_2001, aff_labels_DC_2001, aff_DC_2001 = avgPooling(nodeEmbedding_dc_2001, affiliation_DC_2001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt8vY8gvcteN"
      },
      "outputs": [],
      "source": [
        "visualize_features(aff_embedding_DC_2001, aff_labels_DC_2001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrcejb1GdoZ9"
      },
      "source": [
        "### Affiliation Classical ML Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6Guy2b76e-9"
      },
      "outputs": [],
      "source": [
        "def affiliation_classifier(features, targets, feature_selection = False, regularization=10.0, cross_validation=True, classifier_type='SVC'):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "    features [numpy.ndarray]: NxD Train matrix where N is the number of nodes/affiliations \n",
        "                              and D the number of features\n",
        "    targets [numpy.ndarray]: Nx1 Label vect where each node is labeld with 0,1 or 2\n",
        "                            depending wether they are good, bad or neutral.\n",
        "    train_mask [numpy.ndarray]: NxD Train mask filled with True/False indicating\n",
        "                                which elements are to be used for training or not\n",
        "    test_mask [numpy.ndarray]: Nx1 Test mask filled with True/False indicating\n",
        "                               which elements are to be used for testing or not\n",
        "    feature_selection [bool]: Flag for weather to use feature selction, default is \n",
        "                              False. If an integer number is given than it is used as \n",
        "                              the number of highest ranked features to use\n",
        "    regularization [float]: Regularization parameter\n",
        "    cross__validation [bool]: If True the classifier performs a cv with scope to\n",
        "                              return the best regularization parameter. If False\n",
        "                              the classifier trains the whole dataset normally.\n",
        "    classifier_type [str]: Classifier to use. Possible classifiers : \n",
        "                            Linear SVM          -> \"LinearSVC\"\n",
        "                            RBF kernel SVM      -> \"SVC\"\n",
        "                            Logistic regression -> \"Logistic\"\n",
        "\n",
        "  Outputs:\n",
        "     confmat: confusion matrix\n",
        "     scores: feature ranking scores\n",
        "  \"\"\"\n",
        "  seed = 0 \n",
        "\n",
        "  # split the data into training and testing sets\n",
        "  X_train, X_test, y_train, y_test = train_test_split(features, targets, stratify=targets, test_size=0.25, random_state = seed)\n",
        "\n",
        "  # build and train the ML model\n",
        "  scaler = StandardScaler().fit(X_train)\n",
        "  X_train = scaler.transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  if feature_selection:\n",
        "    selector = SelectKBest(f_classif, k=feature_selection)\n",
        "    X_train = selector.fit_transform(X_train, y_train)\n",
        "    features = selector.get_support(indices=True)\n",
        "    scores = selector.scores_\n",
        "    X_test = X_test[:,features]\n",
        "  \n",
        "  if classifier_type == 'SVC':\n",
        "      model = SVC(C=regularization, random_state=seed, tol=1e-5)\n",
        "  elif classifier_type == 'LinearSVC':\n",
        "      model = LinearSVC(random_state=seed, tol=1e-2, max_iter=1000)\n",
        "  elif classifier_type == 'Logistic':\n",
        "      model = LogisticRegression(random_state=seed, tol=1e-2, max_iter=1000)\n",
        "\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "  if cross_validation:\n",
        "      params = {'C':[0.5, 1.0, 5.0, 10.0, 15.0]}\n",
        "      kf = StratifiedKFold(n_splits=5, shuffle=True, random_state = seed) #set random_state to be 0\n",
        "      clf = GridSearchCV(model, params, scoring='f1_macro', n_jobs=-1, cv=kf)\n",
        "      clf.fit(X_train, y_train)\n",
        "      print('Best hyperparameter:')\n",
        "      print(clf.best_params_)\n",
        "  else:\n",
        "      print('The result of {}'.format(classifier_type))\n",
        "      model.fit(X_train, y_train)\n",
        "\n",
        "      # use the model to predict the labels of the test data\n",
        "      pre_test = model.predict(X_test)\n",
        "      confmat = confusion_matrix(y_test, pre_test, normalize='true')\n",
        "      print('Test accuracy: {}'.format(np.mean(pre_test==y_test)))\n",
        "      print('F1 score: {}'.format(f1_score(y_test, pre_test, average='macro')))\n",
        "\n",
        "      # Display the confusion matrix\n",
        "      plotConfusion(confmat)\n",
        "      if feature_selection:\n",
        "        return confmat, scores\n",
        "      else:\n",
        "        return confmat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JFc0YpjgkLL"
      },
      "source": [
        "### Marvel 2000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHLwHSzngnR9"
      },
      "source": [
        "**Linear SVM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N37H71Yhgplb"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUzhtGHngmhz"
      },
      "outputs": [],
      "source": [
        "affiliation_classifier(aff_embedding_mar_2000, aff_labels_mar_2000, feature_selection = False, regularization=10.0, cross_validation=True, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQbRb-F_gyQq"
      },
      "outputs": [],
      "source": [
        "_ = affiliation_classifier(aff_embedding_mar_2000, aff_labels_mar_2000, feature_selection = False, regularization=0.5, cross_validation=False, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl-o18l2g-hE"
      },
      "source": [
        "**SVM with RBF kernel**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avu795ArhBlK"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gTIP00Yg8J1"
      },
      "outputs": [],
      "source": [
        "affiliation_classifier(aff_embedding_mar_2000, aff_labels_mar_2000, feature_selection = False, regularization=10.0, cross_validation=True, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAhOkT5k6fCN"
      },
      "outputs": [],
      "source": [
        "_ = affiliation_classifier(aff_embedding_mar_2000, aff_labels_mar_2000, feature_selection = False, regularization=10.0, cross_validation=False, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgb_muZ3hNKj"
      },
      "source": [
        "**Logistic regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SJOLC4jhPYz"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1TZPjYYhQ7y"
      },
      "outputs": [],
      "source": [
        "affiliation_classifier(aff_embedding_mar_2000, aff_labels_mar_2000, feature_selection = False, regularization=10.0, cross_validation=True, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVyH7rPwhXFj"
      },
      "outputs": [],
      "source": [
        "_ = affiliation_classifier(aff_embedding_mar_2000, aff_labels_mar_2000, feature_selection = False, regularization=0.5, cross_validation=False, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cl3rXRLh4N9"
      },
      "source": [
        "### Marvel 2001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIZCZ5Hgh75r"
      },
      "source": [
        "**Linear SVM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOdCvwHvh900"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fdHTkIxh7Gr"
      },
      "outputs": [],
      "source": [
        "affiliation_classifier(aff_embedding_mar_2001, aff_labels_mar_2001, feature_selection = False, regularization=10.0, cross_validation=True, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iodjnIpiDSD"
      },
      "outputs": [],
      "source": [
        "_ = affiliation_classifier(aff_embedding_mar_2001, aff_labels_mar_2001, feature_selection = False, regularization=0.5, cross_validation=False, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdVjptTgiI47"
      },
      "source": [
        "**SVM with RBF kernel**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn6kmz8fiLwE"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uC4uycQ7iDUL"
      },
      "outputs": [],
      "source": [
        "affiliation_classifier(aff_embedding_mar_2001, aff_labels_mar_2001, feature_selection = False, regularization=10.0, cross_validation=True, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4Nj-_OaiDWO"
      },
      "outputs": [],
      "source": [
        "_ = affiliation_classifier(aff_embedding_mar_2001, aff_labels_mar_2001, feature_selection = False, regularization=10.0, cross_validation=False, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvZajkWiiVf6"
      },
      "source": [
        "**Logistic regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCKyuxK2iXjc"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oReoTxTniDXk"
      },
      "outputs": [],
      "source": [
        "affiliation_classifier(aff_embedding_mar_2001, aff_labels_mar_2001, feature_selection = False, regularization=10.0, cross_validation=True, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMB2EEMdicar"
      },
      "outputs": [],
      "source": [
        "_ = affiliation_classifier(aff_embedding_mar_2001, aff_labels_mar_2001, feature_selection = False, regularization=0.5, cross_validation=False, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OskmF8V8ihHR"
      },
      "source": [
        "### DC 2000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzAaVB_WijJT"
      },
      "source": [
        "**Linear SVM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb1iZouGitOz"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j_Wbl-silkb"
      },
      "outputs": [],
      "source": [
        "affiliation_classifier(aff_embedding_DC_2000, aff_labels_DC_2000, feature_selection = False, regularization=10.0, cross_validation=True, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xc0N2uOIipg7"
      },
      "outputs": [],
      "source": [
        "_ = affiliation_classifier(aff_embedding_DC_2000, aff_labels_DC_2000, feature_selection = False, regularization=0.5, cross_validation=False, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6Ez7Apvivvs"
      },
      "source": [
        "**SVM with RBF kernel**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRJq-fh6iyFb"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcR6DK-lizST"
      },
      "outputs": [],
      "source": [
        "affiliation_classifier(aff_embedding_DC_2000, aff_labels_DC_2000, feature_selection = False, regularization=10.0, cross_validation=True, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMS0oSCOi3vm"
      },
      "outputs": [],
      "source": [
        "_ = affiliation_classifier(aff_embedding_DC_2000, aff_labels_DC_2000, feature_selection = False, regularization=15.0, cross_validation=False, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUPxp-m5i7fr"
      },
      "source": [
        "**Logistic regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tcVmSW3i-gj"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pig7E5o46fHu"
      },
      "outputs": [],
      "source": [
        "affiliation_classifier(aff_embedding_DC_2000, aff_labels_DC_2000, feature_selection = False, regularization=10.0, cross_validation=True, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSZqRxGCjEen"
      },
      "outputs": [],
      "source": [
        "_ = affiliation_classifier(aff_embedding_DC_2000, aff_labels_DC_2000, feature_selection = False, regularization=0.5, cross_validation=False, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZukhbGikjKb8"
      },
      "source": [
        "### DC 2001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMPRxeB2jMIi"
      },
      "source": [
        "**Linear SVM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgVbnz2tjN2m"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tr6JR5mjEgx"
      },
      "outputs": [],
      "source": [
        "affiliation_classifier(aff_embedding_DC_2001, aff_labels_DC_2001, feature_selection = False, regularization=10.0, cross_validation=True, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBLM7zk1jEi8"
      },
      "outputs": [],
      "source": [
        "_ = affiliation_classifier(aff_embedding_DC_2001, aff_labels_DC_2001, feature_selection = False, regularization=0.5, cross_validation=False, classifier_type='LinearSVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSKaea-Sjbaj"
      },
      "source": [
        "**SVM with RBF kernel**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yacG9tztjdYb"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEl1itEyjEle"
      },
      "outputs": [],
      "source": [
        "affiliation_classifier(aff_embedding_DC_2001, aff_labels_DC_2001, feature_selection = False, regularization=10.0, cross_validation=True, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtdeakN76fKr"
      },
      "outputs": [],
      "source": [
        "_ = affiliation_classifier(aff_embedding_DC_2001, aff_labels_DC_2001, feature_selection = False, regularization=5.0, cross_validation=False, classifier_type='SVC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoQlw2uIjoYc"
      },
      "source": [
        "**Logistic regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FEfbjX9jq6y"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FJQfNuW6fM-"
      },
      "outputs": [],
      "source": [
        "affiliation_classifier(aff_embedding_DC_2001, aff_labels_DC_2001, feature_selection = False, regularization=10.0, cross_validation=True, classifier_type='Logistic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cm8Rwrpy6fPs"
      },
      "outputs": [],
      "source": [
        "_ = affiliation_classifier(aff_embedding_DC_2001, aff_labels_DC_2001, feature_selection = False, regularization=5.0, cross_validation=False, classifier_type='Logistic')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "lovyf3q9XJ3e",
        "T97kgcy1gpd1"
      ],
      "machine_shape": "hm",
      "name": "Copy of network_machine_learning_project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}